# vLLM Scheduler Profiling åŠŸèƒ½

è¿™ä¸ªåŠŸèƒ½å¯ä»¥å¸®åŠ©æ‚¨è¯¦ç»†åˆ†ævLLMè°ƒåº¦å™¨çš„æ€§èƒ½ï¼Œè®°å½•æ¯ä¸ªæ‰¹æ¬¡çš„è°ƒåº¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬prefillæ•°é‡ã€decodeæ•°é‡ã€chunk sizeå’Œè°ƒåº¦æ—¶é—´ç­‰ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ“Š **è¯¦ç»†ç»Ÿè®¡**: è®°å½•æ¯æ‰¹æ¬¡çš„prefill/decodeè¯·æ±‚æ•°é‡
- â±ï¸ **æ€§èƒ½åˆ†æ**: æµ‹é‡è°ƒåº¦å™¨æ‰§è¡Œæ—¶é—´å’Œæ¨¡å‹è¿è¡Œæ—¶é—´
- ğŸ“¦ **Chunk Sizeè¿½è¸ª**: åˆ†æprefillå’Œdecodeçš„token chunkå¤§å°
- ğŸ“ˆ **è¶‹åŠ¿åˆ†æ**: æ”¯æŒæ—¶é—´åºåˆ—åˆ†æå’Œå¯è§†åŒ–
- ğŸ”§ **çµæ´»é…ç½®**: é€šè¿‡ç¯å¢ƒå˜é‡è½»æ¾å¯ç”¨/ç¦ç”¨
- âš¡ **å®Œæ•´æ—¶é—´é“¾è·¯**: ä»è°ƒåº¦åˆ°æ¨¡å‹æ‰§è¡Œçš„å®Œæ•´æ—¶é—´åˆ†æ

## è®°å½•çš„ä¿¡æ¯

æ¯ä¸ªè°ƒåº¦æ‰¹æ¬¡ä¼šè®°å½•ä»¥ä¸‹ä¿¡æ¯:

```json
{
  "batch_id": 123,                    // æ‰¹æ¬¡ID
  "timestamp": 1703123456.789,        // æ—¶é—´æˆ³
  "schedule_duration_ms": 5.2,        // è°ƒåº¦è€—æ—¶(æ¯«ç§’)
  "model_run_duration_ms": 45.8,      // Model Runè€—æ—¶(æ¯«ç§’)
  "total_step_duration_ms": 51.0,     // æ€»Stepè€—æ—¶(æ¯«ç§’)
  "num_prefill_reqs": 3,              // Prefillè¯·æ±‚æ•°
  "num_decode_reqs": 7,               // Decodeè¯·æ±‚æ•°  
  "total_scheduled_tokens": 2048,     // æ€»è°ƒåº¦tokenæ•°
  "prefill_chunk_sizes": [512, 256, 128], // Prefill chunkå¤§å°åˆ—è¡¨
  "decode_chunk_sizes": [1, 1, 1, 1, 1, 1, 1], // Decode chunkå¤§å°åˆ—è¡¨
  "avg_prefill_chunk_size": 298.67,   // å¹³å‡prefill chunkå¤§å°
  "max_prefill_chunk_size": 512,      // æœ€å¤§prefill chunkå¤§å°
  "min_prefill_chunk_size": 128,      // æœ€å°prefill chunkå¤§å°
  "avg_decode_chunk_size": 1.0,       // å¹³å‡decode chunkå¤§å°
  "num_waiting_reqs": 5,              // ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¯·æ±‚æ•°
  "num_running_reqs": 10,             // è¿è¡Œä¸­çš„è¯·æ±‚æ•°
  "kv_cache_usage": 0.75              // KV cacheä½¿ç”¨ç‡
}
```

## ä½¿ç”¨æ–¹æ³•

### 1. å¯ç”¨Profiling

é€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨profilingåŠŸèƒ½:

```bash
export VLLM_ENABLE_SCHEDULER_PROFILING=true
export VLLM_SCHEDULER_PROFILING_LOG=scheduler_profiling.jsonl
export VLLM_SCHEDULER_PROFILING_CONSOLE=true
```

### 2. è¿è¡ŒvLLMæœåŠ¡å™¨

```bash
python -m vllm.entrypoints.openai.api_server \
    --model your_model_name \
    --host 0.0.0.0 \
    --port 8000
```

### 3. å‘é€æµ‹è¯•è¯·æ±‚

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your_model_name",
    "prompt": "Hello, how are you?",
    "max_tokens": 100
  }'
```

### 4. åˆ†æProfilingæ•°æ®

ä½¿ç”¨æä¾›çš„åˆ†æè„šæœ¬:

```bash
python scheduler_profiling_example.py analyze scheduler_profiling.jsonl
```

## ç¯å¢ƒå˜é‡é…ç½®

| ç¯å¢ƒå˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|---------|--------|------|
| `VLLM_ENABLE_SCHEDULER_PROFILING` | `false` | æ˜¯å¦å¯ç”¨profiling |
| `VLLM_SCHEDULER_PROFILING_LOG` | `scheduler_profiling.jsonl` | æ—¥å¿—æ–‡ä»¶è·¯å¾„ |
| `VLLM_SCHEDULER_PROFILING_CONSOLE` | `false` | æ˜¯å¦åœ¨æ§åˆ¶å°è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ |

## åˆ†æå·¥å…·

æä¾›çš„`scheduler_profiling_example.py`è„šæœ¬åŒ…å«:

### åŸºæœ¬ç»Ÿè®¡
- æ€»æ‰¹æ¬¡æ•°
- å¹³å‡/æœ€å¤§/æœ€å°è°ƒåº¦æ—¶é—´
- å¹³å‡prefill/decodeè¯·æ±‚æ•°
- å¹³å‡chunk sizeåˆ†æ

### å¯è§†åŒ–å›¾è¡¨
- è°ƒåº¦æ—¶é—´vsæ¨¡å‹è¿è¡Œæ—¶é—´è¶‹åŠ¿å¯¹æ¯”å›¾
- Prefill vs Decodeè¯·æ±‚æ•°åˆ†å¸ƒ
- æ€»Tokenæ•°è¶‹åŠ¿
- è°ƒåº¦æ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾
- æ¨¡å‹è¿è¡Œæ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾
- è°ƒåº¦æ—¶é—´vsæ¨¡å‹è¿è¡Œæ—¶é—´ç›¸å…³æ€§æ•£ç‚¹å›¾

## å®é™…åº”ç”¨åœºæ™¯

### 1. æ€§èƒ½ä¼˜åŒ–
é€šè¿‡åˆ†æè°ƒåº¦æ—¶é—´è¶‹åŠ¿ï¼Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ:
```bash
# æŸ¥çœ‹è°ƒåº¦æ—¶é—´è¶…è¿‡10msçš„æ‰¹æ¬¡
grep '"schedule_duration_ms":[0-9][0-9]\.' scheduler_profiling.jsonl
```

### 2. Chunk Sizeè°ƒä¼˜
åˆ†æä¸åŒchunk sizeå¯¹æ€§èƒ½çš„å½±å“:
- è§‚å¯Ÿprefill chunk sizeåˆ†å¸ƒ
- å¯¹æ¯”ä¸åŒé…ç½®ä¸‹çš„throughput

### 3. å®¹é‡è§„åˆ’  
é€šè¿‡è¯·æ±‚æ•°é‡è¶‹åŠ¿é¢„æµ‹èµ„æºéœ€æ±‚:
- ç›‘æ§waiting queueé•¿åº¦
- åˆ†æKV cacheä½¿ç”¨ç‡

### 4. å¼‚å¸¸æ£€æµ‹
è¯†åˆ«è°ƒåº¦å¼‚å¸¸æƒ…å†µ:
- è°ƒåº¦æ—¶é—´çªç„¶å¢é•¿
- è¯·æ±‚ç§¯å‹åœ¨waiting queue

## ç¤ºä¾‹åˆ†æ

```python
import json
import pandas as pd

# è¯»å–profilingæ•°æ®
data = []
with open('scheduler_profiling.jsonl', 'r') as f:
    for line in f:
        data.append(json.loads(line))

df = pd.DataFrame(data)

# åˆ†æè°ƒåº¦æ•ˆç‡
print(f"å¹³å‡è°ƒåº¦æ—¶é—´: {df['schedule_duration_ms'].mean():.2f}ms")
print(f"P95è°ƒåº¦æ—¶é—´: {df['schedule_duration_ms'].quantile(0.95):.2f}ms")

# åˆ†ææ¨¡å‹è¿è¡Œæ•ˆç‡
if 'model_run_duration_ms' in df.columns:
    print(f"å¹³å‡æ¨¡å‹è¿è¡Œæ—¶é—´: {df['model_run_duration_ms'].mean():.2f}ms")
    print(f"P95æ¨¡å‹è¿è¡Œæ—¶é—´: {df['model_run_duration_ms'].quantile(0.95):.2f}ms")
    print(f"è°ƒåº¦å æ€»æ—¶é—´æ¯”ä¾‹: {(df['schedule_duration_ms'].mean() / df['total_step_duration_ms'].mean()):.2%}")

# åˆ†æthroughput
total_time_ms = df['total_step_duration_ms'].sum() if 'total_step_duration_ms' in df.columns else df['schedule_duration_ms'].sum()
print(f"å¹³å‡tokens/ç§’: {df['total_scheduled_tokens'].sum() / total_time_ms * 1000:.2f}")

# åˆ†æè´Ÿè½½ç‰¹å¾
print(f"Prefillæ¯”ä¾‹: {df['num_prefill_reqs'].sum() / (df['num_prefill_reqs'].sum() + df['num_decode_reqs'].sum()):.2%}")
```

## æ³¨æ„äº‹é¡¹

1. **æ€§èƒ½å½±å“**: Profilingä¼šå¢åŠ å°‘é‡å¼€é”€ï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒä½¿ç”¨
2. **ç£ç›˜ç©ºé—´**: é•¿æ—¶é—´è¿è¡Œä¼šäº§ç”Ÿå¤§é‡æ—¥å¿—ï¼Œæ³¨æ„ç£ç›˜ç©ºé—´
3. **æ•°æ®æ ¼å¼**: ä½¿ç”¨JSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¾¿äºæµå¼å¤„ç†
4. **æ—¶åŒº**: æ—¶é—´æˆ³ä½¿ç”¨Unixæ—¶é—´æˆ³ï¼Œæ³¨æ„æ—¶åŒºè½¬æ¢

## æ•…éšœæ’é™¤

### æ—¥å¿—æ–‡ä»¶ä¸ºç©º
- æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®
- ç¡®è®¤vLLMè¿›ç¨‹æœ‰å†™æ–‡ä»¶æƒé™
- æ£€æŸ¥æ˜¯å¦æœ‰å®é™…è°ƒåº¦å‘ç”Ÿ

### åˆ†æè„šæœ¬æŠ¥é”™
- å®‰è£…ä¾èµ–: `pip install pandas matplotlib`
- æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
- ç¡®è®¤Pythonç‰ˆæœ¬å…¼å®¹æ€§

### æ§åˆ¶å°æ— è¾“å‡º
- æ£€æŸ¥`VLLM_SCHEDULER_PROFILING_CONSOLE`ç¯å¢ƒå˜é‡
- æŸ¥çœ‹vLLMæ—¥å¿—çº§åˆ«è®¾ç½® 