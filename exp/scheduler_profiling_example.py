#!/usr/bin/env python3
"""
vLLM Scheduler Profiling ç¤ºä¾‹è„šæœ¬

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•å¯ç”¨å’Œä½¿ç”¨vLLM schedulerçš„profilingåŠŸèƒ½æ¥åˆ†æè°ƒåº¦æ€§èƒ½ã€‚
"""

import os
import subprocess
import sys
import json
from pathlib import Path
import numpy as np
import math

try:
    import pandas as pd
    import matplotlib.pyplot as plt
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…ä¾èµ–: pip install pandas matplotlib")
    sys.exit(1)

def enable_scheduler_profiling():
    """å¯ç”¨scheduler profiling"""
    # è®¾ç½®ç¯å¢ƒå˜é‡æ¥å¯ç”¨profiling
    os.environ['VLLM_ENABLE_SCHEDULER_PROFILING'] = 'true'
    os.environ['VLLM_SCHEDULER_PROFILING_LOG'] = 'scheduler_profiling.jsonl'
    os.environ['VLLM_SCHEDULER_PROFILING_CONSOLE'] = 'true'
    
    print("âœ… Scheduler Profiling å·²å¯ç”¨")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {os.environ['VLLM_SCHEDULER_PROFILING_LOG']}")
    print(f"ğŸ“Ÿ æ§åˆ¶å°è¾“å‡º: å·²å¯ç”¨")

def analyze_profiling_data(log_file_or_dir='profiling_result'):
    """åˆ†æprofilingæ•°æ®"""
    log_path = Path(log_file_or_dir)
    
    # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾å…¶ä¸­çš„jsonlæ–‡ä»¶
    if log_path.is_dir():
        jsonl_files = list(log_path.glob('*.jsonl'))
        if not jsonl_files:
            print(f"âŒ åœ¨ç›®å½• {log_file_or_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°jsonlæ–‡ä»¶")
            return
        # ä½¿ç”¨å…¨éƒ¨jsonlæ–‡ä»¶
        log_files = jsonl_files
        print(f"ğŸ“ æ‰¾åˆ°ç›®å½•: {log_file_or_dir}")
        print(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶: {len(jsonl_files)} ä¸ª")
    else:
        log_file = log_path
        if not log_file.exists():
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ {log_file} ä¸å­˜åœ¨")
            return
        # å•æ–‡ä»¶åˆ†æ
        log_files = [log_file]
        print(f"ğŸ“„ ä½¿ç”¨å•ä¸ªæ–‡ä»¶: {log_file}")
    
    # è¯»å–æ•°æ®
    data = []
    batch_id_offset = 0
    # æ ¹æ®æ–‡ä»¶åä¸­çš„æ•°å­—æ’åºï¼›è‹¥åªæœ‰ä¸€ä¸ªæ–‡ä»¶æˆ–è§£æå¤±è´¥åˆ™æŒ‰åç§°æ’åº/ä¿æŒä¸å˜
    if len(log_files) > 1:
        try:
            log_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
        except Exception:
            log_files.sort(key=lambda x: x.name)
    # print(log_files)
    for log_file in log_files:
        with open(log_file, 'r', encoding='utf-8') as f:
            # å»æ‰æ¯ä¸ªæ–‡ä»¶çš„å‰10è¡Œå’Œå10è¡Œ
            lines = f.readlines()
            for line in lines[10:-10]:
                try:
                    entry = json.loads(line.strip())
                    # å¯¹batch idè¿›è¡Œé‡æ–°å¤„ç†
                    if 'batch_id' in entry:
                        entry['batch_id'] += batch_id_offset
                    data.append(entry)
                except json.JSONDecodeError:
                    continue
        batch_id_offset += len(lines[10:-10])

    # æ•°æ®æ¸…æ´—
    # å°†è°ƒåº¦æ—¶é—´åœ¨300msä»¥ä¸Šçš„æ•°æ®åˆ é™¤
    data = [item for item in data if item['schedule_duration_ms'] < 300]
    # # å°†è¿è¡Œæ—¶é—´åœ¨200msä»¥ä¸Šçš„æ•°æ®åˆ é™¤
    data = [item for item in data if item['model_run_duration_ms'] < 200]
    
    if not data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„profilingæ•°æ®")
        return
    
    df = pd.DataFrame(data)
    print(f"âœ… æˆåŠŸè¯»å– {len(data)} æ¡profilingæ•°æ®")
    
    # åŸºäº chunk_sizes ä¸­å€¼ä¸º 1 çš„ä¸ªæ•°è®¡ç®— Decode è¯·æ±‚æ•°
    if 'chunk_sizes' in df.columns:
        def _count_decode_reqs(sizes):
            if isinstance(sizes, list):
                return sum(1 for s in sizes if s == 1)
            try:
                return 1 if sizes == 1 else 0
            except Exception:
                return 0
        df['num_decode_reqs'] = df['chunk_sizes'].apply(_count_decode_reqs)

        def _count_prefill_reqs(sizes):
            if isinstance(sizes, list):
                return sum(1 for s in sizes if s > 1)
            try:
                return 1 if sizes > 1 else 0
            except Exception:
                return 0
        df['num_prefill_reqs'] = df['chunk_sizes'].apply(_count_prefill_reqs)

    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š Scheduler Profiling åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    print(f"ğŸ“ˆ æ€»æ‰¹æ¬¡æ•°: {len(df)}")
    print(f"â±ï¸  å¹³å‡è°ƒåº¦æ—¶é—´: {df['schedule_duration_ms'].mean():.2f}ms")
    print(f"â±ï¸  æœ€å¤§è°ƒåº¦æ—¶é—´: {df['schedule_duration_ms'].max():.2f}ms")
    print(f"â±ï¸  æœ€å°è°ƒåº¦æ—¶é—´: {df['schedule_duration_ms'].min():.2f}ms")
    
    if 'model_run_duration_ms' in df.columns:
        print(f"âš¡ å¹³å‡Model Runæ—¶é—´: {df['model_run_duration_ms'].mean():.2f}ms")
        print(f"âš¡ æœ€å¤§Model Runæ—¶é—´: {df['model_run_duration_ms'].max():.2f}ms")
        print(f"âš¡ æœ€å°Model Runæ—¶é—´: {df['model_run_duration_ms'].min():.2f}ms")
    
    # è¯·æ±‚æ•°ç»Ÿè®¡ï¼ˆæ”¯æŒæ–°æ—§æ•°æ®æ ¼å¼ï¼‰
    if 'num_prefill_reqs' in df.columns:
        print(f"\nğŸ”¢ å¹³å‡Prefillè¯·æ±‚æ•°: {df['num_prefill_reqs'].mean():.2f}")
    if 'num_decode_reqs' in df.columns:
        print(f"ğŸ”¢ å¹³å‡Decodeè¯·æ±‚æ•°: {df['num_decode_reqs'].mean():.2f}")
    
    print(f"ğŸ”¢ å¹³å‡æ€»Tokenæ•°: {df['total_scheduled_tokens'].mean():.2f}")
    
    # Chunk sizeåˆ†æï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'chunk_sizes' in df.columns:
        # è®¡ç®—å¹³å‡chunk size
        chunk_sizes = []
        for sizes in df['chunk_sizes']:
            if isinstance(sizes, list) and sizes:
                chunk_sizes.extend(sizes)
        if chunk_sizes:
            print(f"\nğŸ“¦ å¹³å‡Chunk Size: {sum(chunk_sizes)/len(chunk_sizes):.2f}")
    
    # æ–°å¢ï¼šKV cache hitåˆ†æ
    if 'all_cached_tokens' in df.columns:
        # è®¡ç®—KV cacheç»Ÿè®¡
        all_cached_tokens = []
        for cached_tokens in df['all_cached_tokens']:
            if isinstance(cached_tokens, list):
                # è¿‡æ»¤æ‰-1å€¼ï¼ˆæœªè®¾ç½®çš„cached tokensï¼‰
                valid_cached = [t for t in cached_tokens if t >= 0]
                if valid_cached:
                    all_cached_tokens.extend(valid_cached)
        
        if all_cached_tokens:
            print(f"\nğŸ—‚ï¸  KV Cacheç»Ÿè®¡:")
            print(f"   å¹³å‡æ¯è¯·æ±‚ç¼“å­˜å‘½ä¸­: {sum(all_cached_tokens)/len(all_cached_tokens):.2f}")
            print(f"   æœ€å¤§ç¼“å­˜å‘½ä¸­æ•°: {max(all_cached_tokens)}")
            print(f"   ç¼“å­˜å‘½ä¸­ç‡(æŒ‰è¯·æ±‚): {len([t for t in all_cached_tokens if t > 0])/len(all_cached_tokens)*100:.1f}%")
    
    # æ–°å¢ï¼šcomputed tokensåˆ†æ
    if 'all_computed_tokens' in df.columns:
        all_computed_tokens = []
        for computed_tokens in df['all_computed_tokens']:
            if isinstance(computed_tokens, list):
                all_computed_tokens.extend(computed_tokens)
        
        if all_computed_tokens:
            print(f"\nğŸ”„ Computed Tokensç»Ÿè®¡:")
            print(f"   å¹³å‡å·²è®¡ç®—Tokenæ•°: {sum(all_computed_tokens)/len(all_computed_tokens):.2f}")
            print(f"   æœ€å¤§å·²è®¡ç®—Tokenæ•°: {max(all_computed_tokens)}")
            print(f"   æœ€å°å·²è®¡ç®—Tokenæ•°: {min(all_computed_tokens)}")
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    create_profiling_plots(df)

def create_profiling_plots(df):
    """åˆ›å»ºprofilingæ•°æ®çš„å¯è§†åŒ–å›¾è¡¨"""
    # ç§»é™¤ä¸­æ–‡å­—ä½“è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤è‹±æ–‡å­—ä½“
    plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    # æ£€æŸ¥æ˜¯å¦æœ‰model runæ—¶é—´æ•°æ®å’Œcached tokensæ•°æ®
    has_model_run_data = 'model_run_duration_ms' in df.columns
    has_cached_data = 'all_cached_tokens' in df.columns
    
    # åŠ¨æ€è°ƒæ•´å­å›¾å¸ƒå±€
    if has_model_run_data and has_cached_data:
        fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    elif has_model_run_data or has_cached_data:
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    else:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    fig.suptitle('vLLM Scheduler Profiling Analysis', fontsize=16)
    
    # æ—¶é—´åˆ†å¸ƒ
    if has_model_run_data:
        axes[0, 0].plot(df['batch_id'], df['schedule_duration_ms'], label='Schedule Time', alpha=0.7)
        axes[0, 0].plot(df['batch_id'], df['model_run_duration_ms'], label='Model Run Time', alpha=0.7)
        # è®¡ç®—æ€»æ—¶é—´ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
        # if 'total_step_duration_ms' in df.columns:
        #     axes[0, 0].plot(df['batch_id'], df['total_step_duration_ms'], label='Total Time', alpha=0.7)
        # else:
        #     total_time = df['schedule_duration_ms'] + df['model_run_duration_ms']
        #     axes[0, 0].plot(df['batch_id'], total_time, label='Total Time', alpha=0.7)
        axes[0, 0].set_title('Time Trend Comparison')
        axes[0, 0].legend()
    else:
        axes[0, 0].plot(df['batch_id'], df['schedule_duration_ms'])
        axes[0, 0].set_title('Schedule Time Trend')
    axes[0, 0].set_xlabel('Batch ID')
    axes[0, 0].set_ylabel('Time (ms)')
    axes[0, 0].grid(True)
    
    # Prefill vs Decodeè¯·æ±‚æ•°ï¼ˆå¦‚æœæ•°æ®å­˜åœ¨ï¼‰
    ax_idx = (0, 1)
    axes[ax_idx].plot(df['batch_id'], df['num_prefill_reqs'], label='Prefill Requests')
    axes[ax_idx].plot(df['batch_id'], df['num_decode_reqs'], label='Decode Requests')
    axes[ax_idx].set_xlabel('Batch ID')
    axes[ax_idx].set_ylabel('Number of Requests')
    axes[ax_idx].legend()
    axes[ax_idx].grid(True)

    # chunk_sizes vs è¿è¡Œæ—¶é—´åˆ†å¸ƒ
    ax_idx = (0, 2)
    # å¤„ç†chunk_sizesæ•°æ® - è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„æ€»chunk size
    prefill_chunk_totals = df['chunk_sizes'].apply(lambda x: sum(x) if isinstance(x, list) else x)
    
    # æ ¹æ®chunk_sizesçš„é•¿åº¦è®¾ç½®é¢œè‰²ï¼Œé•¿åº¦è¶Šå¤§é¢œè‰²è¶Šæ·±
    import matplotlib.cm as cm
    import matplotlib.colors as mcolors
    
    chunk_lengths = []
    for chunk_sizes in df['chunk_sizes']:
        if isinstance(chunk_sizes, list):
            chunk_lengths.append(len(chunk_sizes))
        else:
            chunk_lengths.append(1)  # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œé»˜è®¤é•¿åº¦ä¸º1
    
    # åˆ›å»ºé¢œè‰²æ˜ å°„ï¼Œä»æµ…åˆ°æ·±
    if chunk_lengths:
        min_len = min(chunk_lengths)
        max_len = max(chunk_lengths)
        if max_len > min_len:
            # ä½¿ç”¨Bluesé¢œè‰²æ˜ å°„ï¼Œæ•°å€¼è¶Šå¤§é¢œè‰²è¶Šæ·±
            norm = mcolors.Normalize(vmin=min_len, vmax=max_len)
            cmap = cm.Blues
            colors = [cmap(norm(length)) for length in chunk_lengths]
        else:
            colors = ['blue'] * len(chunk_lengths)  # æ‰€æœ‰é•¿åº¦ç›¸åŒæ—¶ä½¿ç”¨ç»Ÿä¸€é¢œè‰²
    else:
        colors = ['blue']  # é»˜è®¤é¢œè‰²
    axes[ax_idx].scatter(prefill_chunk_totals, df['model_run_duration_ms'], c=colors, alpha=0.6, s=20)

    # # å¤„ç†chunk_sizesæ•°æ® - è®¡ç®—chunk_sizesä¸­å¤§äº1çš„ä¸ªæ•°ï¼Œå¤§äº1çš„ä¸ºæ©™è‰²
    
    # # è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„æ€»scheduled tokens
    # total_scheduled_tokens = df['total_scheduled_tokens']
    
    # # æ ¹æ®chunk_sizesä¸­å¤§äº1çš„å…ƒç´ ä¸ªæ•°æ¥ç€è‰²
    # colors = []
    # for chunk_sizes in df['chunk_sizes']:
    #     if isinstance(chunk_sizes, list):
    #         # ç»Ÿè®¡chunk_sizesä¸­å¤§äº1çš„å…ƒç´ ä¸ªæ•°
    #         large_chunks_count = sum(1 for size in chunk_sizes if size > 1)
    #         if large_chunks_count > 1:
    #             colors.append('orange')  # æœ‰å¤§äº1çš„chunkä½¿ç”¨æ©™è‰²
    #         else:
    #             colors.append('blue')    # å…¨ä¸º1çš„chunkä½¿ç”¨è“è‰²
    #     else:
    #         # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œæ ¹æ®æ•°å€¼åˆ¤æ–­
    #         colors.append('orange' if chunk_sizes > 1 else 'blue')
    
    # # ç»˜åˆ¶æ•£ç‚¹å›¾
    # axes[ax_idx].scatter(total_scheduled_tokens, df['model_run_duration_ms'], 
    #                     c=colors, alpha=0.6, s=20)
    
    axes[ax_idx].set_title('Num Scheduled Tokens vs Model Run Time')
    axes[ax_idx].set_xlabel('Num Scheduled Tokens')
    axes[ax_idx].set_ylabel('Model Run Time (ms)')
    axes[ax_idx].grid(True)
    
    # æ‹Ÿåˆï¼šçº¿æ€§ä¸äºŒæ¬¡å¤šé¡¹å¼ï¼Œå¹¶ç»™å‡ºå‡½æ•°è¡¨è¾¾å¼ä¸R^2
    try:
        x_series = pd.to_numeric(prefill_chunk_totals, errors='coerce')
        y_series = pd.to_numeric(df['model_run_duration_ms'], errors='coerce')
        valid_mask = x_series.notna() & y_series.notna()
        x = x_series[valid_mask].to_numpy()
        y = y_series[valid_mask].to_numpy()
        if x.size >= 3:
            # çº¿æ€§æ‹Ÿåˆ y = m*x + c
            coeffs_lin = np.polyfit(x, y, 1)
            m, c = coeffs_lin[0], coeffs_lin[1]
            p_lin = np.poly1d(coeffs_lin)
            y_pred_lin = p_lin(x)
            ss_res_lin = np.sum((y - y_pred_lin) ** 2)
            ss_tot = np.sum((y - y.mean()) ** 2)
            r2_lin = 1 - ss_res_lin / ss_tot if ss_tot > 0 else 0.0
            
            # äºŒæ¬¡æ‹Ÿåˆ y = a*x^2 + b*x + c2
            coeffs_quad = np.polyfit(x, y, 2)
            a2, b2, c2 = coeffs_quad[0], coeffs_quad[1], coeffs_quad[2]
            p_quad = np.poly1d(coeffs_quad)
            y_pred_quad = p_quad(x)
            ss_res_quad = np.sum((y - y_pred_quad) ** 2)
            r2_quad = 1 - ss_res_quad / ss_tot if ss_tot > 0 else 0.0
            
            # é€‰æ‹©æ›´ä¼˜æ¨¡å‹ï¼ˆæŒ‰R^2ï¼‰å¹¶åœ¨å›¾ä¸Šå åŠ 
            xs = np.linspace(x.min(), x.max(), 200)
            if r2_quad > r2_lin + 0.2:
                axes[ax_idx].plot(xs, p_quad(xs), color='red', linewidth=2, label=f'Quad Fit (R^2={r2_quad:.3f})')
                chosen = 'äºŒæ¬¡'
            else:
                axes[ax_idx].plot(xs, p_lin(xs), color='red', linewidth=2, label=f'Linear Fit (R^2={r2_lin:.3f})')
                chosen = 'çº¿æ€§'
            axes[ax_idx].legend()
            
            # æ§åˆ¶å°è¾“å‡ºæ‹Ÿåˆè¡¨è¾¾å¼
            print("\nğŸ“ Chunk Size ä¸ Model Run Time æ‹Ÿåˆè¡¨è¾¾å¼")
            print(f"çº¿æ€§: y = {c:.6f} + {m:.6f} * x, R^2 = {r2_lin:.6f}")
            print(f"äºŒæ¬¡: y = {c2:.6f} + {b2:.6f} * x + {a2:.6f} * x^2, R^2 = {r2_quad:.6f}")
            print(f"â†’ é€‰æ‹©: {chosen} æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸ æ‹Ÿåˆå¤±è´¥: {e}")

    # num_decode_reqs vs è¿è¡Œæ—¶é—´åˆ†å¸ƒ
    ax_idx = (1, 0)
    axes[ax_idx].scatter(df['num_decode_reqs'], df['model_run_duration_ms'], alpha=0.6, s=20)
    axes[ax_idx].set_title('Num Decode Requests vs Model Run Time')
    axes[ax_idx].set_xlabel('Num Decode Requests')
    axes[ax_idx].set_ylabel('Model Run Time (ms)')
    axes[ax_idx].grid(True)
    
    # num_prefill_reqs vs è¿è¡Œæ—¶é—´åˆ†å¸ƒ
    ax_idx = (1, 1)
    axes[ax_idx].scatter(df['num_prefill_reqs'], df['model_run_duration_ms'], alpha=0.6, s=20)
    axes[ax_idx].set_title('Num Prefill Requests vs Model Run Time')
    axes[ax_idx].set_xlabel('Num Prefill Requests')
    axes[ax_idx].set_ylabel('Model Run Time (ms)')
    axes[ax_idx].grid(True)

    # æ§åˆ¶prefill_chunk_totalså…¨ä¸º1ï¼ŒæŸ¥çœ‹num_decode_reqs vs è¿è¡Œæ—¶é—´åˆ†å¸ƒ
    ax_idx = (1, 2)
    num_decode_reqs = []
    model_run_duration_ms = []
    for _, row in df.iterrows():
        sizes = row.get('chunk_sizes', [])
        if isinstance(sizes, list) and sum(sizes) == len(sizes):
            num_decode_reqs.append(row['num_decode_reqs'])
            model_run_duration_ms.append(row['model_run_duration_ms'])
    axes[ax_idx].scatter(num_decode_reqs, model_run_duration_ms, alpha=0.6, s=20)
    axes[ax_idx].set_title('Decode Only Requests vs Model Run Time')
    axes[ax_idx].set_xlabel('Num Decode Requests')
    axes[ax_idx].set_ylabel('Model Run Time (ms)')
    axes[ax_idx].grid(True)

    # batch_size, chunk_size, model_run_duration_msçƒ­åŠ›å›¾
    ax_idx = (2, 0)
    try:
        if 'chunk_sizes' in df.columns and 'model_run_duration_ms' in df.columns:
            # ä»chunk_sizesæ¨å¯¼batch_sizeä¸å¹³å‡chunk_size
            def _batch_size_from_chunks(sizes):
                if isinstance(sizes, list):
                    return len(sizes)
                return np.nan

            def _avg_chunk_size(sizes):
                if isinstance(sizes, list) and len(sizes) > 0:
                    return sum(sizes)
                if isinstance(sizes, (int, float)):
                    return float(sizes)
                return np.nan

            df['_batch_size_est'] = df['chunk_sizes'].apply(_batch_size_from_chunks)
            df['_chunk_size_est'] = df['chunk_sizes'].apply(_avg_chunk_size)

            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®èŒƒå›´
            valid_df = df[
                (df['_batch_size_est'] >= 1) & (df['_batch_size_est'] <= 120) &
                (df['_chunk_size_est'] >= 1) & (df['_chunk_size_est'] <= 4096) &
                (df['model_run_duration_ms'].notna())
            ].copy()

            if not valid_df.empty:
                # å°†æ•°å€¼åˆ†æ¡£ï¼Œä½¿ç”¨æ›´åˆç†çš„æ¡£ä½æ•°é‡
                batch_bins = 12  # batch_size 1-120 åˆ†ä¸º12æ¡£
                chunk_bins = 16  # chunk_size 1-4096 åˆ†ä¸º16æ¡£
                
                # åˆ›å»ºåˆ†æ¡£è¾¹ç•Œ
                batch_edges = np.linspace(1, 120, batch_bins + 1)
                chunk_edges = np.linspace(1, 4096, chunk_bins + 1)
                
                # å°†æ•°æ®åˆ†é…åˆ°å¯¹åº”çš„æ¡£ä½
                valid_df['_batch_bin'] = pd.cut(valid_df['_batch_size_est'], bins=batch_edges, include_lowest=True, labels=False)
                valid_df['_chunk_bin'] = pd.cut(valid_df['_chunk_size_est'], bins=chunk_edges, include_lowest=True, labels=False)

                # åˆ›å»ºé€è§†è¡¨
                pivot = valid_df.groupby(['_chunk_bin', '_batch_bin'])['model_run_duration_ms'].mean().unstack(fill_value=np.nan)

                # ç¡®ä¿æ‰€æœ‰æ¡£ä½éƒ½å­˜åœ¨ï¼ˆå¡«å……ç¼ºå¤±çš„æ¡£ä½ï¼‰
                full_batch_range = range(batch_bins)
                full_chunk_range = range(chunk_bins)
                pivot = pivot.reindex(index=full_chunk_range, columns=full_batch_range)

                # ä»…åœ¨3x3å¸ƒå±€ä¸‹ç»˜åˆ¶åˆ°(2,0)ï¼Œå¦åˆ™è·³è¿‡é¿å…å½±å“ä¸»å›¾ä¿å­˜
                if hasattr(axes, 'shape') and axes.shape[0] >= 3 and axes.shape[1] >= 1:
                    ax_heat = axes[2, 0]
                    
                    # ä½¿ç”¨masked arrayå¤„ç†NaNå€¼ï¼Œè¿™æ ·æ²¡æœ‰æ•°æ®çš„åœ°æ–¹ä¼šæ˜¾ç¤ºä¸ºç™½è‰²
                    masked_data = np.ma.masked_invalid(pivot.values)
                    
                    im = ax_heat.imshow(
                        masked_data, 
                        origin='lower', 
                        cmap='viridis',  # ä½¿ç”¨viridisé¢œè‰²æ˜ å°„ï¼Œå¯¹ç¼ºå¤±å€¼æ›´å‹å¥½
                        aspect='auto',
                        interpolation='nearest'
                    )
                    
                    ax_heat.set_title('Heatmap: Batch Size vs Avg Chunk Size')
                    ax_heat.set_xlabel('Batch Size')
                    ax_heat.set_ylabel('Avg Chunk Size')
                    
                    # è®¾ç½®åæ ‡è½´æ ‡ç­¾ï¼Œæ˜¾ç¤ºå®é™…çš„æ•°å€¼èŒƒå›´
                    batch_labels = [f'{int(batch_edges[i])}-{int(batch_edges[i+1])}' for i in range(0, len(batch_edges)-1, 2)]
                    chunk_labels = [f'{int(chunk_edges[i])}-{int(chunk_edges[i+1])}' for i in range(0, len(chunk_edges)-1, 3)]
                    
                    ax_heat.set_xticks(range(0, batch_bins, 2))
                    ax_heat.set_xticklabels(batch_labels, rotation=45, ha='right')
                    ax_heat.set_yticks(range(0, chunk_bins, 3))
                    ax_heat.set_yticklabels(chunk_labels)
                    
                    # æ·»åŠ é¢œè‰²æ¡
                    cbar = plt.colorbar(im, ax=ax_heat)
                    cbar.set_label('Model Run Time (ms)')
                    
                    print(f"ğŸ“Š çƒ­åŠ›å›¾ç”ŸæˆæˆåŠŸï¼Œæ•°æ®ç‚¹æ•°: {len(valid_df)}")
                else:
                    print('â„¹ï¸ å­å›¾å¸ƒå±€ä¸è¶³ä»¥æ”¾ç½®çƒ­åŠ›å›¾ï¼Œå·²è·³è¿‡ç»˜åˆ¶ï¼ˆéœ€è¦3x3å¸ƒå±€ï¼‰ã€‚')
            else:
                print('âš ï¸ åœ¨æŒ‡å®šèŒƒå›´å†…æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•ç”Ÿæˆçƒ­åŠ›å›¾')
    except Exception as e:
        print(f"âš ï¸ çƒ­åŠ›å›¾ç»˜åˆ¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
  
    # å¦‚æœæœ‰KV cacheæ•°æ®ï¼Œæ·»åŠ é¢å¤–çš„å›¾è¡¨
    if has_cached_data and 'all_cached_tokens' in df.columns:
        # è®¡ç®—æ¯æ‰¹æ¬¡çš„cache hitç»Ÿè®¡
        cache_hit_totals = []
        cache_hit_rates = []
        
        for idx, row in df.iterrows():
            cached_tokens = row.get('all_cached_tokens', [])
            if isinstance(cached_tokens, list):
                valid_cached = [t for t in cached_tokens if t >= 0]
                cache_hit_totals.append(sum(valid_cached))
                if valid_cached:
                    cache_hit_rates.append(len([t for t in valid_cached if t > 0]) / len(valid_cached) * 100)
                else:
                    cache_hit_rates.append(0)
        
        if cache_hit_totals:
            # Cache Hitæ€»æ•°è¶‹åŠ¿
            if has_model_run_data and has_cached_data and axes.shape[0] >= 3:
                # å¦‚æœæœ‰3x3å¸ƒå±€ï¼Œä½¿ç”¨æœ€åä¸€è¡Œæœ€åä¸€åˆ—
                ax_idx = (2, 2)
                axes[ax_idx].plot(df['batch_id'], cache_hit_totals, color='green')
                axes[ax_idx].set_title('KV Cache Hit Tokens Trend')
                axes[ax_idx].set_xlabel('Batch ID')
                axes[ax_idx].set_ylabel('Cache Hit Tokens')
                axes[ax_idx].grid(True)
    
    plt.tight_layout()
    plt.savefig('scheduler_profiling_analysis_a100.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å›¾è¡¨å·²ä¿å­˜ä¸º: scheduler_profiling_analysis_a100.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ vLLM Scheduler Profiling å·¥å…·")
    print("=" * 40)
    
    if len(sys.argv) < 2:
        print("""
ä½¿ç”¨æ–¹æ³•:
  python scheduler_profiling_example.py enable                    # å¯ç”¨profiling
  python scheduler_profiling_example.py analyze                   # åˆ†æprofiling_resultç›®å½•ä¸­çš„æ•°æ®
  python scheduler_profiling_example.py analyze <file_or_dir>     # åˆ†ææŒ‡å®šæ–‡ä»¶æˆ–ç›®å½•ä¸­çš„æ•°æ®
        """)
        return
    
    command = sys.argv[1]
    
    if command == 'enable':
        enable_scheduler_profiling()
        print("""
ğŸ”§ ç°åœ¨ä½ å¯ä»¥è¿è¡ŒvLLMæœåŠ¡å™¨:

export VLLM_ENABLE_SCHEDULER_PROFILING=true
export VLLM_SCHEDULER_PROFILING_LOG=scheduler_profiling.jsonl
export VLLM_SCHEDULER_PROFILING_CONSOLE=true

python -m vllm.entrypoints.openai.api_server \\
    --model your_model_name \\
    --host 0.0.0.0 \\
    --port 8000

ç„¶åå‘é€è¯·æ±‚è¿›è¡Œæµ‹è¯•ï¼Œprofilingæ•°æ®å°†è¢«è®°å½•åˆ° scheduler_profiling.jsonl æ–‡ä»¶ä¸­ã€‚

ğŸ” åˆ†ææ•°æ®æ—¶ï¼Œè¯·ä½¿ç”¨:
python scheduler_profiling_example.py analyze profiling_result
        """)
    
    elif command == 'analyze':
        log_path = sys.argv[2] if len(sys.argv) > 2 else 'profiling_result'
        analyze_profiling_data(log_path)
    
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")

if __name__ == '__main__':
    main() 