#!/usr/bin/env python3
"""
OSDIæŠ•ç¨¿ç”¨æ•£ç‚¹å›¾ç»˜åˆ¶è„šæœ¬
ç»˜åˆ¶åœ¨å›ºå®šTotal Tokenæ•°é‡ä¸‹ï¼ŒLatencyéšBatch Sizeå˜åŒ–çš„æ•£ç‚¹å›¾

ä½¿ç”¨æ–¹æ³•:
python osdi_latency_vs_batch_scatter.py <æ•°æ®æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„>
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from pathlib import Path
from collections import defaultdict

# è®¾ç½®matplotlibå‚æ•°ä»¥è·å¾—OSDIè®ºæ–‡çº§åˆ«çš„é«˜è´¨é‡è¾“å‡º
plt.rcParams.update({
    'font.size': 12,
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.figsize': (12, 8),
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1,
    'axes.unicode_minus': False,
    'lines.linewidth': 2,
    'lines.markersize': 6
})

def load_profiling_data(log_file_or_dir):
    """åŠ è½½profilingæ•°æ® - ä»paper_heatmap.pyå¤ç”¨"""
    log_path = Path(log_file_or_dir)
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if log_path.is_dir():
        jsonl_files = list(log_path.glob('*.jsonl'))
        if not jsonl_files:
            raise FileNotFoundError(f"åœ¨ç›®å½• {log_file_or_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°jsonlæ–‡ä»¶")
        log_files = jsonl_files
        print(f"ğŸ“ æ‰¾åˆ°ç›®å½•: {log_file_or_dir}, æ–‡ä»¶æ•°: {len(jsonl_files)}")
    else:
        if not log_path.exists():
            raise FileNotFoundError(f"æ—¥å¿—æ–‡ä»¶ {log_file_or_dir} ä¸å­˜åœ¨")
        log_files = [log_path]
        print(f"ğŸ“„ ä½¿ç”¨å•ä¸ªæ–‡ä»¶: {log_path}")
    
    # è¯»å–å’Œåˆå¹¶æ•°æ®
    data = []
    batch_id_offset = 0
    
    # æŒ‰æ–‡ä»¶åæ’åº
    if len(log_files) > 1:
        try:
            log_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
        except Exception:
            log_files.sort(key=lambda x: x.name)
    
    for log_file in log_files:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # è·³è¿‡å‰å10è¡Œä»¥é¿å…ä¸ç¨³å®šæ•°æ®
            for line in lines[10:-10]:
                try:
                    entry = json.loads(line.strip())
                    if 'batch_id' in entry:
                        entry['batch_id'] += batch_id_offset
                    data.append(entry)
                except json.JSONDecodeError:
                    continue
        batch_id_offset += len(lines[10:-10])
    
    # æ•°æ®æ¸…æ´—ï¼šç§»é™¤å¼‚å¸¸å€¼
    print(f"åŸå§‹æ•°æ®ç‚¹æ•°: {len(data)}")
    data = [item for item in data if item.get('schedule_duration_ms', 0) < 300]
    data = [item for item in data if item.get('model_run_duration_ms', 0) < 200]
    print(f"æ¸…æ´—åæ•°æ®ç‚¹æ•°: {len(data)}")
    
    if not data:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„profilingæ•°æ®")
    
    return pd.DataFrame(data)

def extract_features_for_scatter(df):
    """ä¸ºæ•£ç‚¹å›¾æå–ç‰¹å¾ï¼šbatch_size, total_tokens, latency"""
    def get_batch_size(sizes):
        if isinstance(sizes, list):
            return len(sizes)
        return np.nan
    
    def get_total_tokens(sizes):
        if isinstance(sizes, list):
            return sum(sizes)
        if isinstance(sizes, (int, float)):
            return float(sizes)
        return np.nan
    
    df['batch_size'] = df['chunk_sizes'].apply(get_batch_size)
    df['total_tokens'] = df['chunk_sizes'].apply(get_total_tokens)
    df['latency_ms'] = df['model_run_duration_ms']
    
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_df = df[
        (df['batch_size'] >= 1) & (df['batch_size'] <= 120) &
        (df['total_tokens'] >= 50) & (df['total_tokens'] <= 8192) &
        (df['latency_ms'].notna()) & (df['latency_ms'] > 0)
    ].copy()
    
    print(f"æå–ç‰¹å¾åæœ‰æ•ˆæ•°æ®ç‚¹æ•°: {len(valid_df)}")
    print(f"Total tokensèŒƒå›´: {valid_df['total_tokens'].min():.0f} - {valid_df['total_tokens'].max():.0f}")
    print(f"Batch sizeèŒƒå›´: {valid_df['batch_size'].min():.0f} - {valid_df['batch_size'].max():.0f}")
    print(f"LatencyèŒƒå›´: {valid_df['latency_ms'].min():.2f} - {valid_df['latency_ms'].max():.2f} ms")
    
    return valid_df

def filter_data_by_token_ranges(valid_df, target_tokens=[128, 256, 512, 1024, 2048, 4096], tolerance=0.15):
    """
    æ ¹æ®ç›®æ ‡tokenæ•°é‡è¿‡æ»¤æ•°æ®
    tolerance: å…è®¸çš„ç›¸å¯¹è¯¯å·®èŒƒå›´ï¼Œä¾‹å¦‚0.15è¡¨ç¤ºÂ±15%
    """
    filtered_data = {}
    
    for target in target_tokens:
        # è®¡ç®—å®¹å¿èŒƒå›´
        lower_bound = target * (1 - tolerance)
        upper_bound = target * (1 + tolerance)
        
        # è¿‡æ»¤æ•°æ®
        mask = (valid_df['total_tokens'] >= lower_bound) & (valid_df['total_tokens'] <= upper_bound)
        filtered_subset = valid_df[mask].copy()
        
        if len(filtered_subset) > 0:
            filtered_data[target] = filtered_subset
            print(f"Token={target}: æ‰¾åˆ° {len(filtered_subset)} ä¸ªæ•°æ®ç‚¹ (èŒƒå›´: {lower_bound:.0f}-{upper_bound:.0f})")
        else:
            print(f"Token={target}: æœªæ‰¾åˆ°æ•°æ®ç‚¹ (èŒƒå›´: {lower_bound:.0f}-{upper_bound:.0f})")
    
    return filtered_data

def compute_statistics(filtered_data):
    """è®¡ç®—æ¯ä¸ªtokençº§åˆ«å’Œbatch sizeçš„ç»Ÿè®¡ä¿¡æ¯"""
    stats_data = {}
    
    for token_count, data in filtered_data.items():
        # æŒ‰batch_sizeåˆ†ç»„å¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        grouped = data.groupby('batch_size')['latency_ms'].agg([
            'mean', 'std', 'count', 'median'
        ]).reset_index()
        
        # åªä¿ç•™æœ‰è¶³å¤Ÿæ•°æ®ç‚¹çš„ç»„ï¼ˆè‡³å°‘3ä¸ªæ•°æ®ç‚¹ï¼‰
        grouped = grouped[grouped['count'] >= 3]
        
        if len(grouped) > 0:
            stats_data[token_count] = grouped
            print(f"Token={token_count}: {len(grouped)} ä¸ªæœ‰æ•ˆbatch sizeç»„")
    
    return stats_data

def plot_latency_vs_batch_scatter(stats_data, filtered_data, output_path='osdi_latency_vs_batch_scatter.pdf'):
    """ç»˜åˆ¶OSDIæŠ•ç¨¿ç”¨çš„é«˜è´¨é‡æ•£ç‚¹å›¾"""
    
    # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ - ä½¿ç”¨ä¸“ä¸šçš„é¢œè‰²æ­é…
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    markers = ['o', 's', '^', 'D', 'v', '<']
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    token_counts = sorted(stats_data.keys())
    
    for i, token_count in enumerate(token_counts):
        if token_count not in stats_data:
            continue
            
        stats = stats_data[token_count]
        raw_data = filtered_data[token_count]
        
        color = colors[i % len(colors)]
        marker = markers[i % len(markers)]
        
        # ç»˜åˆ¶åŸå§‹æ•°æ®ç‚¹ï¼ˆåŠé€æ˜ï¼‰
        ax.scatter(raw_data['batch_size'], raw_data['latency_ms'], 
                  alpha=0.3, s=20, c=color, marker=marker)
        
        # ç»˜åˆ¶ç»Ÿè®¡å‡å€¼ç‚¹ï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰
        ax.scatter(stats['batch_size'], stats['mean'], 
                  s=80, c=color, marker=marker, 
                  label=f'Total Tokens = {token_count}', 
                  edgecolors='black', linewidth=1)
        
        # æ·»åŠ è¯¯å·®çº¿
        ax.errorbar(stats['batch_size'], stats['mean'], yerr=stats['std'], 
                   fmt='none', ecolor=color, alpha=0.7, capsize=3)
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('Batch Size', fontsize=14, fontweight='bold')
    ax.set_ylabel('Latency (ms)', fontsize=14, fontweight='bold')
    ax.set_title('Latency vs Batch Size for Different Total Token Counts', 
                fontsize=16, fontweight='bold', pad=20)
    
    # è®¾ç½®ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # è®¾ç½®å›¾ä¾‹
    legend = ax.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    legend.get_frame().set_facecolor('white')
    legend.get_frame().set_alpha(0.9)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    ax.set_xlim(0, max([data['batch_size'].max() for data in filtered_data.values()]) + 5)
    ax.set_ylim(0, max([data['latency_ms'].max() for data in filtered_data.values()]) * 1.1)
    
    # ç´§å‡‘å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
    base_name = output_path.replace('.pdf', '')
    plt.savefig(f'{base_name}.pdf', format='pdf', dpi=300, bbox_inches='tight')
    plt.savefig(f'{base_name}.png', format='png', dpi=300, bbox_inches='tight')
    plt.savefig(f'{base_name}.eps', format='eps', dpi=300, bbox_inches='tight')
    
    print(f"âœ… OSDIæ•£ç‚¹å›¾å·²ä¿å­˜:")
    print(f"   PDF: {base_name}.pdf")
    print(f"   PNG: {base_name}.png") 
    print(f"   EPS: {base_name}.eps")
    
    plt.show()

def generate_summary_statistics(stats_data, filtered_data):
    """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ•°æ®æ±‡æ€»ç»Ÿè®¡:")
    print("=" * 60)
    
    for token_count in sorted(stats_data.keys()):
        stats = stats_data[token_count]
        raw_data = filtered_data[token_count]
        
        print(f"\nTotal Tokens = {token_count}:")
        print(f"  æ•°æ®ç‚¹æ€»æ•°: {len(raw_data)}")
        print(f"  Batch sizeèŒƒå›´: {raw_data['batch_size'].min():.0f} - {raw_data['batch_size'].max():.0f}")
        print(f"  å¹³å‡Latency: {raw_data['latency_ms'].mean():.2f} Â± {raw_data['latency_ms'].std():.2f} ms")
        print(f"  LatencyèŒƒå›´: {raw_data['latency_ms'].min():.2f} - {raw_data['latency_ms'].max():.2f} ms")
        
        # åˆ†æbatch sizeå¯¹latencyçš„å½±å“
        if len(stats) >= 3:
            batch_sizes = stats['batch_size'].values
            latencies = stats['mean'].values
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            correlation = np.corrcoef(batch_sizes, latencies)[0, 1]
            print(f"  Batch sizeä¸Latencyç›¸å…³ç³»æ•°: {correlation:.3f}")
            
            # è®¡ç®—å¢é•¿ç‡ï¼ˆä»æœ€å°åˆ°æœ€å¤§batch sizeï¼‰
            if len(latencies) >= 2:
                growth_rate = (latencies[-1] - latencies[0]) / latencies[0] * 100
                print(f"  Latencyå¢é•¿ç‡: {growth_rate:.1f}%")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python osdi_latency_vs_batch_scatter.py <æ•°æ®æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„>")
        print("ç¤ºä¾‹: python osdi_latency_vs_batch_scatter.py profiling_result")
        return
    
    data_path = sys.argv[1]
    
    try:
        print("ğŸš€ å¼€å§‹ç”ŸæˆOSDIæŠ•ç¨¿ç”¨æ•£ç‚¹å›¾")
        print("=" * 50)
        
        # 1. åŠ è½½æ•°æ®
        df = load_profiling_data(data_path)
        
        # 2. æå–ç‰¹å¾
        valid_df = extract_features_for_scatter(df)
        
        if len(valid_df) < 50:
            print("âš ï¸ æœ‰æ•ˆæ•°æ®ç‚¹è¾ƒå°‘ï¼Œå¯èƒ½å½±å“å›¾è¡¨è´¨é‡")
        
        # 3. æŒ‰tokenæ•°é‡è¿‡æ»¤æ•°æ®
        target_tokens = [128, 256, 512, 1024, 2048, 4096]
        filtered_data = filter_data_by_token_ranges(valid_df, target_tokens)
        
        if not filtered_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
            return
        
        # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats_data = compute_statistics(filtered_data)
        
        # 5. ç»˜åˆ¶æ•£ç‚¹å›¾
        plot_latency_vs_batch_scatter(stats_data, filtered_data)
        
        # 6. ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        generate_summary_statistics(stats_data, filtered_data)
        
        print("\nğŸ‰ OSDIæŠ•ç¨¿å›¾è¡¨ç”Ÿæˆå®Œæˆï¼")
        print("=" * 50)
        print("ğŸ“Š ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print("   - osdi_latency_vs_batch_scatter.pdf (æŠ•ç¨¿ç”¨ä¸»å›¾)")
        print("   - osdi_latency_vs_batch_scatter.png (é¢„è§ˆç”¨)")
        print("   - osdi_latency_vs_batch_scatter.eps (é«˜è´¨é‡çŸ¢é‡å›¾)")
        print("\nğŸ“ OSDIæŠ•ç¨¿å»ºè®®:")
        print("   - å›¾è¡¨æ˜¾ç¤ºäº†åœ¨å›ºå®štokenæ•°é‡ä¸‹batch sizeå¯¹latencyçš„å½±å“")
        print("   - è¯¯å·®çº¿è¡¨ç¤ºæ ‡å‡†å·®ï¼Œå±•ç¤ºäº†æ•°æ®çš„å˜å¼‚æ€§")
        print("   - åŠé€æ˜ç‚¹æ˜¾ç¤ºåŸå§‹æ•°æ®åˆ†å¸ƒï¼Œå®å¿ƒç‚¹æ˜¾ç¤ºç»Ÿè®¡å‡å€¼")
        print("   - å»ºè®®åœ¨è®ºæ–‡ä¸­è®¨è®ºbatch sizeçš„é¥±å’Œæ•ˆåº”")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main() 