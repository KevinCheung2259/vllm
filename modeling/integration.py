#!/usr/bin/env python3
"""
ä¸ scheduler_profiling_example.py çš„é›†æˆæ¨¡å—
æä¾›ç®€åŒ–çš„æ¥å£æ¥ä½¿ç”¨ååé¥±å’Œå»ºæ¨¡åŠŸèƒ½
"""

import sys
import os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# æ·»åŠ modelingç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from performance_model import ThroughputSaturationModel, StableClusterModel


def analyze_with_modeling(log_file_or_dir='profiling_result', use_stable_model=False, suffix='', save_outputs=True):
    """
    ä½¿ç”¨ååé¥±å’Œæ¨¡å‹åˆ†æprofilingæ•°æ®
    
    Args:
        log_file_or_dir: æ—¥å¿—æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
        use_stable_model: æ˜¯å¦ä½¿ç”¨ç¨³å®šé›†ç¾¤æ¨¡å‹
        suffix: æ–‡ä»¶ååç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„åˆ†æç»“æœ
        save_outputs: æ˜¯å¦ä¿å­˜æ¨¡å‹å’Œå›¾ç‰‡åˆ°æ–‡ä»¶ç³»ç»Ÿ
        
    Returns:
        (model, df) å…ƒç»„
    """
    model_type = "ç¨³å®šé›†ç¾¤è°ƒåº¦æ¨¡å‹" if use_stable_model else "ååé¥±å’Œæ¨¡å‹"
    print(f"ğŸ”¬ ä½¿ç”¨{model_type}åˆ†æprofilingæ•°æ®")
    print("=" * 50)
    
    # è¯»å–æ•°æ®
    df = read_profiling_data(log_file_or_dir)
    if df is None or df.empty:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return None, None
    
    print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡profilingæ•°æ®")
    
    # åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹
    try:
        if use_stable_model:
            model = StableClusterModel(verbose=True)
        else:
            model = ThroughputSaturationModel(verbose=True)
            
        model.fit(df)
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        if use_stable_model:
            print_stable_model_insights(model)
        else:
            print_model_insights(model)
        
        # ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ä¿å­˜ï¼‰
        if save_outputs:
            if use_stable_model:
                create_stable_model_plots(model, df, suffix)
            else:
                create_modeling_plots(model, df, suffix)

        # æµ‹è¯•æ¨¡å‹æ€§èƒ½
        test_model_performance(model)
        
        return model, df
        
    except Exception as e:
        print(f"âŒ å»ºæ¨¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, df


def test_model_performance(model):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    import time
    
    test_points = [(3, 256), (16, 2048), (32, 4096)]
    print("\nğŸ” æ¨¡å‹æ€§èƒ½æµ‹è¯•:")
    print("-" * 40)
    
    for B, S in test_points:
        start_time = time.perf_counter()
        pred = model.predict(B, S)
        end_time = time.perf_counter()
        predict_cost = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        print(f"Batch={B:2d}, Tokens={S:4d} -> {pred:.2f} ms (é¢„æµ‹è€—æ—¶: {predict_cost:.3f} ms)")


def read_profiling_data(log_file_or_dir):
    """è¯»å–profilingæ•°æ®ï¼ˆå¤ç”¨åŸè„šæœ¬é€»è¾‘ï¼‰"""
    import json
    from pathlib import Path
    
    log_path = Path(log_file_or_dir)
    
    # æŸ¥æ‰¾jsonlæ–‡ä»¶
    if log_path.is_dir():
        jsonl_files = list(log_path.glob('*.jsonl'))
        if not jsonl_files:
            print(f"âŒ åœ¨ç›®å½• {log_file_or_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°jsonlæ–‡ä»¶")
            return None
        log_files = jsonl_files
        print(f"ğŸ“ æ‰¾åˆ°ç›®å½•: {log_file_or_dir}")
        print(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶: {len(jsonl_files)} ä¸ª")
    else:
        if not log_path.exists():
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ {log_path} ä¸å­˜åœ¨")
            return None
        log_files = [log_path]
        print(f"ğŸ“„ ä½¿ç”¨å•ä¸ªæ–‡ä»¶: {log_path}")
    
    # è¯»å–æ•°æ®
    data = []
    batch_id_offset = 0
    
    # æ’åºæ–‡ä»¶
    if len(log_files) > 1:
        try:
            log_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
        except Exception:
            log_files.sort(key=lambda x: x.name)
    
    for log_file in log_files:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # å»æ‰å‰å10è¡Œ
            for line in lines[10:-10]:
                try:
                    entry = json.loads(line.strip())
                    if 'batch_id' in entry:
                        entry['batch_id'] += batch_id_offset
                    data.append(entry)
                except json.JSONDecodeError:
                    continue
        batch_id_offset += len(lines[10:-10])
    
    if not data:
        return None
    
    # æ•°æ®æ¸…æ´—
    data = [item for item in data if item.get('schedule_duration_ms', 0) < 200]
    data = [item for item in data if item.get('model_run_duration_ms', 0) < 200]
    
    df = pd.DataFrame(data)
    
    # è®¡ç®—decodeå’Œprefillè¯·æ±‚æ•°ï¼ˆå¤ç”¨åŸè„šæœ¬é€»è¾‘ï¼‰
    if 'chunk_sizes' in df.columns:
        def _count_decode_reqs(sizes):
            if isinstance(sizes, list):
                return sum(1 for s in sizes if s == 1)
            try:
                return 1 if sizes == 1 else 0
            except Exception:
                return 0
        
        def _count_prefill_reqs(sizes):
            if isinstance(sizes, list):
                return sum(1 for s in sizes if s > 1)
            try:
                return 1 if sizes > 1 else 0
            except Exception:
                return 0
        
        df['num_decode_reqs'] = df['chunk_sizes'].apply(_count_decode_reqs)
        df['num_prefill_reqs'] = df['chunk_sizes'].apply(_count_prefill_reqs)
    
    return df


def print_model_insights(model):
    """æ‰“å°æ¨¡å‹æ·±åº¦åˆ†æ"""
    print("\nğŸ” æ¨¡å‹æ·±åº¦åˆ†æ")
    print("=" * 50)
    
    params = model.params
    param_names = model.param_names
    
    # ç³»ç»Ÿç‰¹æ€§åˆ†æ
    P_max, k_B, k_S = params[0], params[1], params[2]
    w_0, w_1 = params[3], params[4]
    tau_0, tau_B, tau_S = params[5], params[6], params[7]
    
    print(f"ğŸš€ ç³»ç»Ÿååç‰¹æ€§:")
    print(f"   å³°å€¼ååé‡: {P_max:.2f} tokens/ms")
    print(f"   è¾¾åˆ°50%é¥±å’Œçš„batch_size: {-np.log(0.5)/k_B:.1f}")
    print(f"   è¾¾åˆ°50%é¥±å’Œçš„tokenæ•°: {-np.log(0.5)/k_S:.0f}")
    
    print(f"\nâš¡ å·¥ä½œé‡ç‰¹æ€§:")
    print(f"   åŸºç¡€å·¥ä½œé‡: {w_0:.3f}")
    print(f"   æ¯tokenå·¥ä½œé‡: {w_1:.6f}")
    if w_0 > 0:
        print(f"   å›ºå®šå¼€é”€å¯¹åº”tokenæ•°: {w_0/w_1:.0f}")
    
    print(f"\nğŸ•’ å»¶è¿Ÿæ„æˆ:")
    print(f"   åŸºç¡€å»¶è¿Ÿ: {tau_0:.2f} ms")
    print(f"   æ¯batchå¼€é”€: {tau_B:.3f} ms")
    print(f"   æ¯tokenå¼€é”€: {tau_S:.6f} ms")
    
    # å…¸å‹åœºæ™¯é¢„æµ‹
    print(f"\nğŸ“Š å…¸å‹åœºæ™¯é¢„æµ‹:")
    scenarios = [
        ("å°decodeæ‰¹æ¬¡", 4, 4),
        ("ä¸­decodeæ‰¹æ¬¡", 16, 16), 
        ("å¤§decodeæ‰¹æ¬¡", 64, 64),
        ("å°prefill", 2, 128),
        ("å¤§prefill", 1, 1024)
    ]
    
    for name, B, S in scenarios:
        latency = model.predict(B, S)
        print(f"   {name:12s}: B={B:2d}, S={S:4d} -> {latency:.1f} ms")


def create_modeling_plots(model, df, suffix=''):
    """åˆ›å»ºå»ºæ¨¡ç›¸å…³çš„å¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ“Š ç”Ÿæˆå»ºæ¨¡å¯è§†åŒ–å›¾è¡¨...")
    
    # æ„å»ºæ–‡ä»¶å
    suffix_str = f"_{suffix}" if suffix else ""
    
    # åˆ›å»ºç­‰é«˜çº¿å›¾
    fig1 = model.plot_contour(
        batch_range=(1, 64),
        token_range=(1, 1024),
        save_path=f'./modeling/modeling_contour{suffix_str}.png'
    )
    
    # åˆ›å»ºæ®‹å·®åˆ†æå›¾
    fig2 = model.plot_residuals(df)
    plt.savefig(f'./modeling/modeling_residuals{suffix_str}.png', dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Residual analysis plot saved: ./modeling/modeling_residuals{suffix_str}.png")
    
    # Create performance surface plot
    create_3d_surface(model, suffix)
    
    plt.show()





def create_3d_surface(model, suffix=''):
    """åˆ›å»º3Dæ€§èƒ½æ›²é¢å›¾"""
    import numpy as np
    from mpl_toolkits.mplot3d import Axes3D
    
    # æ„å»ºæ–‡ä»¶å
    suffix_str = f"_{suffix}" if suffix else ""
    
    # åˆ›å»ºç½‘æ ¼
    B_range = np.linspace(1, 32, 30)
    S_range = np.linspace(1, 512, 30)
    B_mesh, S_mesh = np.meshgrid(B_range, S_range)
    
    # é¢„æµ‹å»¶è¿Ÿ
    T_mesh = model.predict(B_mesh.flatten(), S_mesh.flatten()).reshape(B_mesh.shape)
    
    # ç»˜åˆ¶3Dæ›²é¢
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    surf = ax.plot_surface(B_mesh, S_mesh, T_mesh, 
                          cmap='viridis', alpha=0.9, 
                          linewidth=0, antialiased=True)
    
    ax.set_xlabel('Batch Size')
    ax.set_ylabel('Total Tokens')
    ax.set_zlabel('Latency (ms)')
    ax.set_title('vLLM Performance 3D Surface\n(Throughput Saturation Model)')
    
    # æ·»åŠ é¢œè‰²æ¡
    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=20)
    
    plt.savefig(f'./modeling/modeling_3d_surface{suffix_str}.png', dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š 3D performance surface plot saved: ./modeling/modeling_3d_surface{suffix_str}.png")





def compare_models(df, suffix='', save_outputs=True):
    """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½"""
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    import numpy as np
    
    print("\nğŸ“ˆ æ¨¡å‹å¯¹æ¯”åˆ†æ")
    print("=" * 50)
    
    # æå–ç‰¹å¾
    B = df['chunk_sizes'].apply(lambda x: len(x) if isinstance(x, list) else np.nan)
    S = df['chunk_sizes'].apply(lambda x: sum(x) if isinstance(x, list) else x)
    T = df['model_run_duration_ms']
    
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    mask = B.notna() & S.notna() & T.notna() & (B > 0) & (S > 0) & (T > 0)
    B, S, T = B[mask].values, S[mask].values, T[mask].values
    
    models = {}
    
    # 1. Linear model (S only)
    lr_s = LinearRegression()
    lr_s.fit(S.reshape(-1, 1), T)
    pred_lr_s = lr_s.predict(S.reshape(-1, 1))
    models['Linear(S)'] = {'pred': pred_lr_s, 'r2': lr_s.score(S.reshape(-1, 1), T)}
    
    # 2. Linear model (B+S)
    lr_bs = LinearRegression()
    X_bs = np.column_stack([B, S])
    lr_bs.fit(X_bs, T)
    pred_lr_bs = lr_bs.predict(X_bs)
    models['Linear(B+S)'] = {'pred': pred_lr_bs, 'r2': lr_bs.score(X_bs, T)}
    
    # 3. Polynomial model
    poly_features = PolynomialFeatures(degree=2)
    X_poly = poly_features.fit_transform(X_bs)
    lr_poly = LinearRegression()
    lr_poly.fit(X_poly, T)
    pred_poly = lr_poly.predict(X_poly)
    models['Polynomial'] = {'pred': pred_poly, 'r2': lr_poly.score(X_poly, T)}
    
    # 4. Throughput saturation model
    ts_model = ThroughputSaturationModel(verbose=False)
    ts_model.fit(df)
    B_norm, S_norm, _ = ts_model._normalize_features(B, S)
    pred_ts = ts_model.latency_model((B_norm, S_norm), *ts_model.params)
    models['Throughput Saturation'] = {'pred': pred_ts, 'r2': ts_model.fit_metrics['r2']}
    
    # Print comparison results
    print("Model Performance Comparison (RÂ²):")
    for name, result in models.items():
        print(f"  {name:20s}: {result['r2']:.4f}")
    
    # Plot comparison
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()
    
    for i, (name, result) in enumerate(models.items()):
        ax = axes[i]
        ax.scatter(T, result['pred'], alpha=0.6, s=20)
        ax.plot([T.min(), T.max()], [T.min(), T.max()], 'r--', lw=2)
        ax.set_xlabel('Actual Latency (ms)')
        ax.set_ylabel('Predicted Latency (ms)')
        ax.set_title(f'{name} (RÂ²={result["r2"]:.3f})')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_outputs:
        plt.savefig(f'./modeling/model_comparison_{suffix}.png', dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Model comparison plot saved: ./modeling/model_comparison_{suffix}.png")
    
    plt.show()
    
    return models


def print_stable_model_insights(model):
    """æ‰“å°ç¨³å®šæ¨¡å‹çš„æ·±åº¦åˆ†æ"""
    print("\nğŸ” ç¨³å®šæ¨¡å‹æ·±åº¦åˆ†æ")
    print("=" * 50)
    
    hardware_info = model.get_hardware_capacity()
    
    # ç³»ç»Ÿç‰¹æ€§åˆ†æ
    P_max = hardware_info['peak_throughput_tokens_per_ms']
    k_B = hardware_info['batch_efficiency_factor']
    k_S = hardware_info['token_efficiency_factor']
    tau_B = hardware_info['per_batch_overhead_ms']
    tau_S = hardware_info['per_token_overhead_ms']
    T_base = hardware_info['base_latency_ms']
    
    print(f"ğŸš€ ç³»ç»Ÿååç‰¹æ€§:")
    print(f"   å³°å€¼ååé‡: {P_max:.4f} tokens/ms (ç¨³å®šä¼°è®¡)")
    print(f"   è¾¾åˆ°50%é¥±å’Œçš„batch_size: {hardware_info['batch_50_saturation']:.1f}")
    print(f"   è¾¾åˆ°50%é¥±å’Œçš„tokenæ•°: {hardware_info['token_50_saturation']:.1f}")
    
    print(f"\nâš¡ å»¶è¿Ÿæ„æˆ:")
    print(f"   åŸºç¡€å»¶è¿Ÿ: {T_base:.2f} ms")
    print(f"   æ¯batchå¼€é”€: {tau_B:.3f} ms")
    print(f"   æ¯tokenå¼€é”€: {tau_S:.6f} ms")
    
    print(f"\nğŸ­ ç¡¬ä»¶è¯„åˆ†:")
    print(f"   ç»¼åˆæ€§èƒ½è¯„åˆ†: {hardware_info['hardware_score']:.4f}")
    print(f"   ä¼°è®¡æ–¹æ³•: {hardware_info['estimation_method']}")
    
    # å…¸å‹åœºæ™¯é¢„æµ‹
    print(f"\nğŸ“Š å…¸å‹åœºæ™¯é¢„æµ‹:")
    scenarios = [
        ("å°decodeæ‰¹æ¬¡", 4, 4),
        ("ä¸­decodeæ‰¹æ¬¡", 16, 16), 
        ("å¤§decodeæ‰¹æ¬¡", 64, 64),
        ("å°prefill", 2, 128),
        ("å¤§prefill", 1, 1024)
    ]
    
    for name, B, S in scenarios:
        latency = model.predict(B, S)
        print(f"   {name:12s}: B={B:2d}, S={S:4d} -> {latency:.1f} ms")


def create_stable_model_plots(model, df, suffix=''):
    """åˆ›å»ºç¨³å®šæ¨¡å‹ç›¸å…³çš„å¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ“Š ç”Ÿæˆç¨³å®šæ¨¡å‹å¯è§†åŒ–å›¾è¡¨...")
    
    # æ„å»ºæ–‡ä»¶å
    suffix_str = f"_{suffix}" if suffix else ""
    
    # åˆ›å»ºç­‰é«˜çº¿å›¾ (å¤ç”¨åŸæœ‰å‡½æ•°ï¼Œç¨³å®šæ¨¡å‹ä¹Ÿæœ‰predictæ–¹æ³•)
    try:
        # åˆ›å»ºç­‰é«˜çº¿å›¾
        B_range = np.linspace(1, 64, 50)
        S_range = np.linspace(1, 1024, 50)
        B_mesh, S_mesh = np.meshgrid(B_range, S_range)
        
        # é¢„æµ‹å»¶è¿Ÿ
        T_mesh = model.predict(B_mesh.flatten(), S_mesh.flatten()).reshape(B_mesh.shape)
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # å¡«å……ç­‰é«˜çº¿
        cs1 = ax.contourf(B_mesh, S_mesh, T_mesh, levels=20, cmap='viridis', alpha=0.8)
        cbar1 = plt.colorbar(cs1, ax=ax)
        cbar1.set_label('Model Run Latency (ms)', fontsize=12)
        
        # ç­‰é«˜çº¿
        cs2 = ax.contour(B_mesh, S_mesh, T_mesh, levels=10, colors='white', linewidths=1, alpha=0.7)
        ax.clabel(cs2, inline=True, fontsize=9, fmt='%1.0f ms')
        
        ax.set_xlabel('Batch Size', fontsize=12)
        ax.set_ylabel('Total Tokens', fontsize=12)
        ax.set_title('vLLM Model Run Latency Prediction\n(Stable Cluster Model)', fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.3)
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
        if model.fit_metrics:
            info_text = f"RÂ² = {model.fit_metrics['r2']:.3f}\nRMSE = {model.fit_metrics['rmse']:.1f} ms\nP_max = {model.P_max:.3f} tokens/ms"
            ax.text(0.02, 0.98, info_text, transform=ax.transAxes, 
                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f'./modeling/stable_model_contour{suffix_str}.png', dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Stable model contour plot saved: ./modeling/stable_model_contour{suffix_str}.png")
        
    except Exception as e:
        print(f"âš ï¸  ç­‰é«˜çº¿å›¾åˆ›å»ºå¤±è´¥: {e}")
    
    # åˆ›å»ºæ®‹å·®åˆ†æå›¾
    try:
        create_stable_residuals_plot(model, df, suffix)
    except Exception as e:
        print(f"âš ï¸  æ®‹å·®å›¾åˆ›å»ºå¤±è´¥: {e}")





def create_stable_residuals_plot(model, df, suffix=''):
    """åˆ›å»ºç¨³å®šæ¨¡å‹çš„æ®‹å·®åˆ†æå›¾"""
    # æ„å»ºæ–‡ä»¶å
    suffix_str = f"_{suffix}" if suffix else ""
    
    # è·å–æ•°æ®
    B, S, T = model._preprocess_data(df)
    B_norm, S_norm, _ = model._normalize_features(B, S)
    
    # é¢„æµ‹å€¼
    T_pred = model.stable_latency_model((B_norm, S_norm), *model.params, model.P_max)
    residuals = T - T_pred
    
    # ç»˜å›¾
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # é¢„æµ‹å€¼ vs çœŸå®å€¼
    axes[0].scatter(T_pred, T, alpha=0.6, s=20)
    axes[0].plot([T.min(), T.max()], [T.min(), T.max()], 'r--', lw=2)
    axes[0].set_xlabel('Predicted Latency (ms)')
    axes[0].set_ylabel('Actual Latency (ms)')
    axes[0].set_title('Predicted vs Actual (Stable Model)')
    axes[0].grid(True, alpha=0.3)
    
    # æ®‹å·® vs é¢„æµ‹å€¼
    axes[1].scatter(T_pred, residuals, alpha=0.6, s=20)
    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[1].set_xlabel('Predicted Latency (ms)')
    axes[1].set_ylabel('Residuals (ms)')
    axes[1].set_title('Residuals vs Predicted')
    axes[1].grid(True, alpha=0.3)
    
    # æ®‹å·®ç›´æ–¹å›¾
    axes[2].hist(residuals, bins=30, alpha=0.7, edgecolor='black')
    axes[2].axvline(x=0, color='r', linestyle='--', lw=2)
    axes[2].axvline(x=0, color='r', linestyle='--', lw=2)
    axes[2].set_xlabel('Residuals (ms)')
    axes[2].set_ylabel('Frequency')
    axes[2].set_title('Residuals Distribution')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'./modeling/stable_model_residuals{suffix_str}.png', dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Stable model residuals plot saved: ./modeling/stable_model_residuals{suffix_str}.png")


def analyze_cluster_scheduling(log_file_or_dir='profiling_result', suffix='', save_outputs=True):
    """
    ä¸“é—¨ç”¨äºé›†ç¾¤è°ƒåº¦çš„åˆ†æå‡½æ•°
    
    Args:
        log_file_or_dir: æ—¥å¿—æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
        suffix: æ–‡ä»¶ååç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„åˆ†æç»“æœ
        save_outputs: æ˜¯å¦ä¿å­˜æ¨¡å‹å’Œå›¾ç‰‡åˆ°æ–‡ä»¶ç³»ç»Ÿ
        
    Returns:
        (model, hardware_info, df) å…ƒç»„
    """
    print("ğŸ­ é›†ç¾¤è°ƒåº¦æ€§èƒ½åˆ†æ")
    print("=" * 50)
    
    # ä½¿ç”¨ç¨³å®šæ¨¡å‹è¿›è¡Œåˆ†æ
    model, df = analyze_with_modeling(log_file_or_dir, use_stable_model=True, suffix=suffix, save_outputs=save_outputs)
    
    if model is None:
        return None, None, df
    
    # è·å–ç¡¬ä»¶èƒ½åŠ›ä¿¡æ¯
    hardware_info = model.get_hardware_capacity()
    
    print("\nğŸ­ ç¡¬ä»¶èƒ½åŠ›åˆ†æ (ç”¨äºé›†ç¾¤è°ƒåº¦)")
    print("-" * 40)
    print("ğŸ’¡ å…³é”®è°ƒåº¦å‚æ•°:")
    print(f"  å³°å€¼ååé‡ (P_max): {hardware_info['peak_throughput_tokens_per_ms']:.4f} tokens/ms")
    print(f"  ç¡¬ä»¶æ€§èƒ½è¯„åˆ†: {hardware_info['hardware_score']:.4f}")
    print(f"  Batch 50%é¥±å’Œç‚¹: {hardware_info['batch_50_saturation']:.2f}")
    print(f"  Token 50%é¥±å’Œç‚¹: {hardware_info['token_50_saturation']:.2f}")
    
    print(f"\nğŸ“Š å¼€é”€åˆ†æ:")
    print(f"  åŸºç¡€å»¶è¿Ÿ: {hardware_info['base_latency_ms']:.2f} ms")
    print(f"  æ¯batchå¼€é”€: {hardware_info['per_batch_overhead_ms']:.4f} ms/batch")
    print(f"  æ¯tokenå¼€é”€: {hardware_info['per_token_overhead_ms']:.6f} ms/token")
    
    # è°ƒåº¦å†³ç­–ç¤ºä¾‹
    print("\nâš¡ è°ƒåº¦å†³ç­–ç¤ºä¾‹")
    print("-" * 30)
    scenarios = [
        (30, 512, "ä½å»¶è¿Ÿå®æ—¶åœºæ™¯"),
        (50, 1024, "å®æ—¶æ¨ç†åœºæ™¯"),
        (100, 2048, "æ‰¹å¤„ç†åœºæ™¯"),
        (200, 4096, "é«˜ååæ‰¹å¤„ç†")
    ]
    
    print(f"{'åœºæ™¯':15s} {'ç›®æ ‡å»¶è¿Ÿ':8s} {'Tokené¢„ç®—':8s} {'æ¨èBatch':8s} {'é¢„æµ‹å»¶è¿Ÿ':8s} {'æœ‰æ•ˆåå':10s}")
    print("-" * 75)
    
    for target_latency, token_budget, desc in scenarios:
        config = model.estimate_optimal_batch_config(target_latency, token_budget)
        
        if config:
            print(f"{desc:15s} {target_latency:8.0f} {token_budget:8d} "
                  f"{config['batch_size']:8.0f} {config['predicted_latency_ms']:8.1f} "
                  f"{config['effective_throughput']:10.4f}")
        else:
            print(f"{desc:15s} {target_latency:8.0f} {token_budget:8d} {'æ— è§£':8s} {'N/A':8s} {'N/A':10s}")
    
    return model, hardware_info, df


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("""
ä½¿ç”¨æ–¹æ³•:
  python integration.py analyze [log_file_or_dir] [suffix] [--no-save]     # ä½¿ç”¨åŸå§‹ååé¥±å’Œæ¨¡å‹åˆ†æprofilingæ•°æ®
  python integration.py stable [log_file_or_dir] [suffix] [--no-save]      # ä½¿ç”¨ç¨³å®šé›†ç¾¤è°ƒåº¦æ¨¡å‹åˆ†æ
  python integration.py cluster [log_file_or_dir] [suffix] [--no-save]     # é›†ç¾¤è°ƒåº¦ä¸“é—¨åˆ†æï¼ˆæ¨èç”¨äºå¼‚æ„é›†ç¾¤ï¼‰
  python integration.py compare [log_file_or_dir] [suffix] [--no-save]     # æ¯”è¾ƒä¸åŒæ¨¡å‹æ€§èƒ½
  python integration.py demo                                                # è¿è¡Œæ¼”ç¤º
  
  å‚æ•°è¯´æ˜:
  log_file_or_dir: profilingæ•°æ®è·¯å¾„
  suffix: å¯é€‰çš„æ–‡ä»¶ååç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„åˆ†æç»“æœï¼ˆå¦‚: a100, h100, v100ç­‰ï¼‰
  --no-save: å¯é€‰å‚æ•°ï¼Œæ·»åŠ æ­¤å‚æ•°å°†ä¸ä¿å­˜æ¨¡å‹å’Œå›¾ç‰‡åˆ°æ–‡ä»¶ç³»ç»Ÿ
  
  ç¤ºä¾‹:
  python integration.py analyze profiling_result a100              # åˆ†æå¹¶ä¿å­˜æ‰€æœ‰è¾“å‡º
  python integration.py stable profiling_result h100 --no-save     # åˆ†æä½†ä¸ä¿å­˜æ–‡ä»¶
        """)
        return
    
    command = sys.argv[1]
    save_outputs = '--no-save' not in sys.argv
    
    # è§£æå‚æ•°
    log_path = sys.argv[2] if len(sys.argv) > 2 else 'profiling_result'
    suffix = sys.argv[3] if len(sys.argv) > 3 else 'a100'
    
    # å¤„ç†--no-saveå‚æ•°ä½ç½®
    if suffix == '--no-save':
        suffix = 'a100'
    
    # æ‰§è¡Œå‘½ä»¤
    if command == 'analyze':
        model, df = analyze_with_modeling(log_path, suffix=suffix, save_outputs=save_outputs)
        save_model_if_needed(model, df, save_outputs, f'fitted_model_{suffix}.pkl')
    
    elif command == 'stable':
        model, df = analyze_with_modeling(log_path, use_stable_model=True, suffix=suffix, save_outputs=save_outputs)
        save_model_if_needed(model, df, save_outputs, f'stable_cluster_model_{suffix}.pkl')
    
    elif command == 'cluster':
        model, hardware_info, df = analyze_cluster_scheduling(log_path, suffix=suffix, save_outputs=save_outputs)
        save_model_if_needed(model, df, save_outputs, f'cluster_scheduling_model_{suffix}.pkl')
        if model and save_outputs:
            print_cluster_info(hardware_info)
    
    elif command == 'compare':
        df = read_profiling_data(log_path)
        if df is not None:
            compare_models(df, suffix, save_outputs)
    
    elif command == 'demo':
        from performance_model import demo_usage
        model, df = demo_usage()
        
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
        print("ğŸ’¡ æç¤º: å¯¹äºå¼‚æ„é›†ç¾¤è°ƒåº¦ï¼Œæ¨èä½¿ç”¨ 'cluster' å‘½ä»¤")


def save_model_if_needed(model, df, save_outputs, filename):
    """å¦‚æœéœ€è¦ä¿å­˜ï¼Œåˆ™ä¿å­˜æ¨¡å‹"""
    if model and df is not None and save_outputs:
        model.save_model(f'./modeling/{filename}')
        print(f"\nğŸ’¾ Model saved: {filename}")


def print_cluster_info(hardware_info):
    """æ‰“å°é›†ç¾¤è°ƒåº¦ä¿¡æ¯"""
    print(f"\nğŸ”‘ å…³é”®é›†ç¾¤è°ƒåº¦å‚æ•°:")
    print(f"   å³°å€¼ååé‡: {hardware_info['peak_throughput_tokens_per_ms']:.6f} tokens/ms")
    print(f"   ç¡¬ä»¶æ€§èƒ½è¯„åˆ†: {hardware_info['hardware_score']:.6f}")
    print(f"   æ‰¹æ¬¡æ•ˆç‡ç³»æ•°: {hardware_info['batch_efficiency_factor']:.6f}")
    print(f"   Tokenæ•ˆç‡ç³»æ•°: {hardware_info['token_efficiency_factor']:.6f}")


if __name__ == '__main__':
    main() 