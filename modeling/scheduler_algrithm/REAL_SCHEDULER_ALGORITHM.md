# vLLM çœŸå®è°ƒåº¦åœºæ™¯çš„åå‘ä¼˜åŒ–ç®—æ³•

## ğŸ¯ é—®é¢˜é‡æ–°å®šä¹‰

åŸºäºscheduler.pyçš„å®é™…è°ƒåº¦é€»è¾‘ï¼Œé‡æ–°å®šä¹‰åå‘ä¼˜åŒ–é—®é¢˜ã€‚

### çœŸå®è¾“å…¥ç»“æ„

#### 1. Running Queue (`self.running`)
æ­£åœ¨è¿è¡Œçš„è¯·æ±‚ï¼ŒåŒ…å«ä¸¤ç±»ï¼š
- **Decodeé˜¶æ®µè¯·æ±‚**: `num_computed_tokens >= num_prompt_tokens`
  - æ¯æ­¥åªéœ€è¦1ä¸ªtokenï¼ˆç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼‰
  - ä¼˜å…ˆçº§æœ€é«˜ï¼Œå¿…é¡»ä¼˜å…ˆæ»¡è¶³
- **Chunked Prefillé˜¶æ®µè¯·æ±‚**: `num_computed_tokens < num_prompt_tokens`  
  - éœ€è¦ç»§ç»­å¤„ç†å‰©ä½™çš„prompt tokens
  - å—`long_prefill_threshold`é™åˆ¶å•æ¬¡chunkå¤§å°

#### 2. Waiting Queue (`self.waiting`)
ç­‰å¾…é˜Ÿåˆ—ä¸­çš„æ–°è¯·æ±‚ï¼š
- éƒ½æ˜¯`num_computed_tokens = 0`çš„æ–°è¯·æ±‚
- éœ€è¦å¼€å§‹prefillé˜¶æ®µ
- æŒ‰ä¼˜å…ˆçº§å’Œåˆ°è¾¾é¡ºåºè°ƒåº¦

### çº¦æŸæ¡ä»¶

1. **Tokené¢„ç®—çº¦æŸ**: `total_tokens â‰¤ max_num_scheduled_tokens`
2. **å¹¶å‘é™åˆ¶**: `len(running) + len(new_scheduled) â‰¤ max_num_running_reqs`
3. **é˜¶æ®µçº¦æŸ**: 
   - Decodeè¯·æ±‚: `chunk_size = 1`
   - Prefillè¯·æ±‚: `1 â‰¤ chunk_size â‰¤ min(remaining_tokens, long_prefill_threshold)`
4. **ä¼˜å…ˆçº§çº¦æŸ**: Running > Waitingï¼ŒDecode > Prefill

### è¾“å‡ºç»“æœ
- **Runningè¯·æ±‚è°ƒåº¦**: `Dict[req_id, chunk_size]`
- **Waitingè¯·æ±‚é€‰æ‹©**: `List[selected_req_ids]` 
- **Waitingè¯·æ±‚åˆ†é…**: `Dict[req_id, chunk_size]`
- **æ€§èƒ½é¢„æµ‹**: é¢„æµ‹å»¶è¿Ÿã€å¯è¡Œæ€§ç­‰

## ğŸ§® ç®—æ³•è®¾è®¡

### æ ¸å¿ƒæ€æƒ³

**ä¸‰é˜¶æ®µä¼˜åŒ–ç­–ç•¥**ï¼š
1. **é˜¶æ®µ1**: ä¼°ç®—æœ€ä¼˜batch_sizeå’Œtotal_tokens
2. **é˜¶æ®µ2**: ä¼˜å…ˆè°ƒåº¦runningé˜Ÿåˆ—ï¼ˆdecode > prefillï¼‰
3. **é˜¶æ®µ3**: è´ªå¿ƒé€‰æ‹©å’Œè°ƒåº¦waitingé˜Ÿåˆ—

### è¯¦ç»†ç®—æ³•æµç¨‹

#### é˜¶æ®µ1: å…¨å±€ä¼˜åŒ–
```python
def estimate_optimal_config(target_latency):
    best_config = None
    best_error = inf
    
    # éå†å¯èƒ½çš„batch_size
    for batch_size in range(current_running, max_running + 1):
        # æ•°å€¼æ±‚è§£æœ€ä¼˜tokenæ•°
        optimal_tokens = solve_equation(
            latency_function(batch_size, S) = target_latency
        )
        
        # è¯„ä¼°è¯¥é…ç½®çš„å¯è¡Œæ€§å’Œè¯¯å·®
        config = simulate_scheduling(batch_size, optimal_tokens)
        error = abs(config.predicted_latency - target_latency)
        
        if error < best_error:
            best_config = config
            best_error = error
    
    return best_config
```

#### é˜¶æ®µ2: Runningé˜Ÿåˆ—è°ƒåº¦ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
```python
def schedule_running_requests(running_reqs, token_budget):
    chunk_sizes = {}
    remaining_tokens = token_budget
    
    # 1. ä¼˜å…ˆè°ƒåº¦decodeè¯·æ±‚ï¼ˆå›ºå®š1 tokenï¼‰
    decode_reqs = [req for req in running_reqs if req.is_decode_phase]
    for req in decode_reqs:
        if remaining_tokens >= 1:
            chunk_sizes[req.request_id] = 1
            remaining_tokens -= 1
        else:
            chunk_sizes[req.request_id] = 0
    
    # 2. è°ƒåº¦chunked prefillè¯·æ±‚
    prefill_reqs = [req for req in running_reqs if not req.is_decode_phase]
    prefill_reqs.sort(key=lambda r: (-r.priority, -r.remaining_tokens))
    
    for req in prefill_reqs:
        max_chunk = min(
            req.remaining_prompt_tokens,
            long_prefill_threshold,
            remaining_tokens
        )
        chunk_sizes[req.request_id] = max_chunk
        remaining_tokens -= max_chunk
    
    return chunk_sizes, remaining_tokens
```

#### é˜¶æ®µ3: Waitingé˜Ÿåˆ—é€‰æ‹©ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰
```python
def select_waiting_requests(waiting_reqs, available_tokens, available_slots):
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    sorted_waiting = sorted(waiting_reqs, key=lambda r: -r.priority)
    
    selected = []
    chunk_sizes = {}
    remaining_tokens = available_tokens
    
    for req in sorted_waiting:
        if len(selected) >= available_slots or remaining_tokens <= 0:
            break
        
        # è®¡ç®—æœ€å°å¯åŠ¨æˆæœ¬
        min_chunk = min(64, req.num_prompt_tokens)  # æœ€å°chunk
        if remaining_tokens < min_chunk:
            break
        
        # è´ªå¿ƒåˆ†é…
        allocation = min(
            req.num_prompt_tokens,
            long_prefill_threshold, 
            remaining_tokens
        )
        
        selected.append(req.request_id)
        chunk_sizes[req.request_id] = allocation
        remaining_tokens -= allocation
    
    return selected, chunk_sizes
```

## ğŸ”§ ç®—æ³•ç‰¹ç‚¹

### 1. ç¬¦åˆçœŸå®è°ƒåº¦é€»è¾‘
- **é˜Ÿåˆ—ç»“æ„**: ä¸¥æ ¼æŒ‰ç…§running/waitingåˆ†ç¦»
- **ä¼˜å…ˆçº§ç­–ç•¥**: decode > chunked_prefill > new_prefill
- **çº¦æŸå¤„ç†**: å®Œå…¨ç¬¦åˆvLLMçš„å®é™…é™åˆ¶

### 2. é«˜æ•ˆçš„æœç´¢ç­–ç•¥
- **æœ‰ç•Œæœç´¢**: batch_sizeæœç´¢ç©ºé—´æœ‰é™
- **å¿«é€Ÿæ±‚è§£**: æ¯ä¸ªbatch_sizeä¸‹çš„tokenæ•°å¯å¿«é€Ÿæ±‚è§£
- **æ—©æœŸç»ˆæ­¢**: æ‰¾åˆ°æ»¡è¶³ç²¾åº¦è¦æ±‚çš„è§£å³å¯åœæ­¢

### 3. å®é™…å¯éƒ¨ç½²
- **æ¥å£å…¼å®¹**: è¾“å‡ºæ ¼å¼ç›´æ¥å¯¹åº”scheduleréœ€è¦çš„å†³ç­–
- **å‚æ•°å¯æ§**: æ”¯æŒå„ç§è°ƒåº¦ç­–ç•¥å‚æ•°
- **ç›‘æ§å‹å¥½**: æä¾›è¯¦ç»†çš„åˆ†é…å’Œé¢„æµ‹ä¿¡æ¯

## ğŸ“Š å¤æ‚åº¦åˆ†æ

### æ—¶é—´å¤æ‚åº¦
- **é˜¶æ®µ1**: `O(B_max * log(T_max))` å…¶ä¸­B_maxä¸ºæœ€å¤§batchæœç´¢èŒƒå›´ï¼ŒT_maxä¸ºtokenæœç´¢èŒƒå›´
- **é˜¶æ®µ2**: `O(R * log(R))` å…¶ä¸­Rä¸ºrunningé˜Ÿåˆ—å¤§å°
- **é˜¶æ®µ3**: `O(W * log(W))` å…¶ä¸­Wä¸ºwaitingé˜Ÿåˆ—å¤§å°

### æ€»å¤æ‚åº¦
`O(B_max * log(T_max) + R*log(R) + W*log(W))`

åœ¨å…¸å‹åœºæ™¯ä¸‹ï¼š
- `B_max â‰ˆ 32`ï¼ˆæœ€å¤§å¹¶å‘æ•°ï¼‰
- `T_max â‰ˆ 8192`ï¼ˆæœ€å¤§tokenæ•°ï¼‰
- `R, W â‰¤ 100`ï¼ˆé˜Ÿåˆ—å¤§å°ï¼‰

æ€»è®¡ç®—é‡çº¦ `32 * 13 + 100 * 7 â‰ˆ 1100`æ¬¡æ“ä½œï¼Œ**éå¸¸é«˜æ•ˆ**ï¼

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. SLAä¿è¯
```python
# ä¸ºP99å»¶è¿Ÿä¼˜åŒ–è°ƒåº¦
target_latency = 50.0  # ms
schedule = optimizer.optimize_schedule(running, waiting, target_latency)

# åº”ç”¨è°ƒåº¦å†³ç­–
for req_id, chunk_size in schedule.running_chunk_sizes.items():
    schedule_running_request(req_id, chunk_size)

for req_id in schedule.scheduled_waiting_ids:
    chunk_size = schedule.waiting_chunk_sizes[req_id]
    promote_waiting_to_running(req_id, chunk_size)
```

### 2. è´Ÿè½½è‡ªé€‚åº”
```python
# æ ¹æ®é˜Ÿåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´ç›®æ ‡å»¶è¿Ÿ
queue_length = len(waiting_requests)
if queue_length > 10:
    target_latency = 30.0  # é«˜è´Ÿè½½ä¸‹é™ä½å»¶è¿Ÿ
else:
    target_latency = 50.0  # ä½è´Ÿè½½ä¸‹ä¿è¯è´¨é‡

schedule = optimizer.optimize_schedule(running, waiting, target_latency)
```

### 3. å¤šç›®æ ‡ä¼˜åŒ–
```python
# åŒæ—¶ä¼˜åŒ–å»¶è¿Ÿå’Œååé‡
schedules = []
for target_lat in [20, 30, 40, 50]:
    schedule = optimizer.optimize_schedule(running, waiting, target_lat)
    schedules.append((schedule, target_lat))

# é€‰æ‹©å¸•ç´¯æ‰˜æœ€ä¼˜è§£
best_schedule = choose_pareto_optimal(schedules, latency_weight=0.7, throughput_weight=0.3)
```

## ğŸ” ä¸åŸç®—æ³•å¯¹æ¯”

| ç‰¹æ€§ | åŸç®—æ³• | çœŸå®åœºæ™¯ç®—æ³• |
|------|--------|-------------|
| **é—®é¢˜å»ºæ¨¡** | æŠ½è±¡åŒ–çš„æ‰¹æ¬¡ä¼˜åŒ– | åŸºäºçœŸå®queueç»“æ„ |
| **ä¼˜å…ˆçº§å¤„ç†** | ç®€å•æƒé‡ | ä¸¥æ ¼çš„ä¸‰çº§ä¼˜å…ˆçº§ |
| **çº¦æŸå»ºæ¨¡** | ç†æƒ³åŒ–çº¦æŸ | å®Œæ•´çš„å®é™…çº¦æŸ |
| **è¾“å‡ºæ ¼å¼** | é€šç”¨chunkåˆ†é… | ç›´æ¥å¯¹åº”è°ƒåº¦å†³ç­– |
| **éƒ¨ç½²éš¾åº¦** | éœ€è¦é€‚é… | å³æ’å³ç”¨ |
| **æ€§èƒ½** | O(log T + N log N) | O(B log T + R log R + W log W) |

## ğŸš€ å·¥ç¨‹å®ç°è¦ç‚¹

### 1. ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ
```python
class SchedulerWithOptimizer(Scheduler):
    def __init__(self, ...):
        super().__init__(...)
        self.optimizer = RealSchedulerOptimizer(model_params, config)
    
    def schedule(self) -> SchedulerOutput:
        # ä½¿ç”¨ä¼˜åŒ–å™¨ç”Ÿæˆè°ƒåº¦æ–¹æ¡ˆ
        if self.enable_optimization:
            target_latency = self.compute_target_latency()
            optimal_schedule = self.optimizer.optimize_schedule(
                self.running, self.waiting, target_latency
            )
            return self.apply_optimal_schedule(optimal_schedule)
        else:
            # å›é€€åˆ°åŸå§‹è°ƒåº¦é€»è¾‘
            return super().schedule()
```

### 2. å‚æ•°è‡ªé€‚åº”
```python
# æ ¹æ®å†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°
def update_model_params(self, actual_latencies, predicted_latencies):
    error = mean_squared_error(actual_latencies, predicted_latencies)
    if error > threshold:
        self.model_params = retrain_model(recent_profiling_data)
        self.optimizer.update_params(self.model_params)
```

### 3. ç›‘æ§å’Œè°ƒè¯•
```python
# è¯¦ç»†çš„è°ƒåº¦å†³ç­–æ—¥å¿—
def log_scheduling_decision(self, schedule, target_latency):
    logger.info(f"Optimization result: target={target_latency:.2f}ms, "
               f"predicted={schedule.predicted_latency:.2f}ms, "
               f"running_decisions={schedule.running_chunk_sizes}, "
               f"new_scheduled={len(schedule.scheduled_waiting_ids)}")
```

è¿™ä¸ªç®—æ³•è®¾è®¡**å®Œå…¨ç¬¦åˆvLLMçš„çœŸå®è°ƒåº¦åœºæ™¯**ï¼Œæä¾›äº†é«˜æ•ˆã€å‡†ç¡®ã€å¯éƒ¨ç½²çš„åå‘ä¼˜åŒ–èƒ½åŠ›ï¼ 