#!/usr/bin/env python3
"""
vLLM çœŸå®è°ƒåº¦åœºæ™¯çš„åå‘ä¼˜åŒ–ç®—æ³•

åŸºäºscheduler.pyçš„å®é™…è°ƒåº¦é€»è¾‘ï¼Œè®¾è®¡ç¬¦åˆrunning/waitingé˜Ÿåˆ—ç»“æ„çš„åå‘ä¼˜åŒ–ç®—æ³•ã€‚
"""

import numpy as np
from scipy.optimize import brentq, minimize_scalar
import logging
from typing import List, Tuple, Dict, Optional, NamedTuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class RequestPhase(Enum):
    """è¯·æ±‚é˜¶æ®µ"""
    DECODE = "decode"           # åœ¨decodeé˜¶æ®µçš„runningè¯·æ±‚
    CHUNKED_PREFILL = "chunked_prefill"  # åœ¨chunked prefillé˜¶æ®µçš„runningè¯·æ±‚
    NEW_PREFILL = "new_prefill" # waitingé˜Ÿåˆ—ä¸­çš„æ–°è¯·æ±‚


@dataclass
class RealRequestInfo:
    """çœŸå®çš„è¯·æ±‚ä¿¡æ¯ï¼ˆåŸºäºscheduler.pyï¼‰"""
    request_id: str
    phase: RequestPhase
    
    # åŸºæœ¬tokenä¿¡æ¯
    num_prompt_tokens: int           # æ€»prompté•¿åº¦
    num_computed_tokens: int         # å·²è®¡ç®—çš„tokenæ•°
    num_cached_tokens: int           # ç¼“å­˜å‘½ä¸­çš„tokenæ•°
    
    # è°ƒåº¦ç›¸å…³
    priority: float = 1.0            # ä¼˜å…ˆçº§æƒé‡
    max_chunk_size: int = 512        # æœ€å¤§chunké™åˆ¶
    
    @property
    def remaining_prompt_tokens(self) -> int:
        """å‰©ä½™éœ€è¦prefillçš„tokenæ•°"""
        return max(0, self.num_prompt_tokens - self.num_computed_tokens)
    
    @property
    def is_decode_phase(self) -> bool:
        """æ˜¯å¦å¤„äºdecodeé˜¶æ®µ"""
        return self.num_computed_tokens >= self.num_prompt_tokens


@dataclass
class SchedulingConfig:
    """è°ƒåº¦é…ç½®"""
    max_num_scheduled_tokens: int   # æœ€å¤§tokené¢„ç®—
    max_num_running_reqs: int       # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    long_prefill_threshold: int = 2048  # é•¿prefillåˆ†å—é˜ˆå€¼
    enable_load_aware: bool = False  # æ˜¯å¦å¯ç”¨è´Ÿè½½æ„ŸçŸ¥
    

@dataclass 
class OptimalSchedule:
    """æœ€ä¼˜è°ƒåº¦ç»“æœ"""
    # è°ƒåº¦å†³ç­–
    running_chunk_sizes: Dict[str, int]      # runningè¯·æ±‚çš„chunkåˆ†é…
    scheduled_waiting_ids: List[str]         # è¢«è°ƒåº¦çš„waitingè¯·æ±‚ID
    waiting_chunk_sizes: Dict[str, int]      # waitingè¯·æ±‚çš„chunkåˆ†é…
    
    # é¢„æµ‹ç»“æœ
    total_tokens: int                        # æ€»tokenæ•°
    predicted_latency: float                 # é¢„æµ‹å»¶è¿Ÿ
    
    # ç»Ÿè®¡ä¿¡æ¯
    num_decode_tokens: int                   # decode tokenæ•°
    num_prefill_tokens: int                  # prefill tokenæ•°
    feasible: bool = True                    # æ˜¯å¦å¯è¡Œ


class RealSchedulerOptimizer:
    """çœŸå®è°ƒåº¦åœºæ™¯çš„åå‘ä¼˜åŒ–å™¨"""
    
    def __init__(self, model_params: Dict, config: SchedulingConfig, verbose: bool = False):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            model_params: æ€§èƒ½æ¨¡å‹å‚æ•°
            config: è°ƒåº¦é…ç½®
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.model_params = model_params
        self.config = config
        self.verbose = verbose
        
        # æå–æ¨¡å‹å‚æ•°
        self.P_max = model_params['P_max']
        self.k_B = model_params['k_B'] 
        self.k_S = model_params['k_S']
        self.w_0 = model_params['w_0']
        self.w_1 = model_params['w_1']
        self.tau_0 = model_params['tau_0']
        self.tau_B = model_params['tau_B']
        self.tau_S = model_params['tau_S']
        
        if verbose:
            logger.info("çœŸå®è°ƒåº¦ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")
    
    def _latency_function(self, batch_size: int, total_tokens: int) -> float:
        """è®¡ç®—å»¶è¿Ÿ"""
        if total_tokens <= 0 or batch_size <= 0:
            return float('inf')
            
        # ååé‡è®¡ç®—
        thr = self.P_max * (1 - np.exp(-self.k_B * batch_size)) * (1 - np.exp(-self.k_S * total_tokens))
        if thr <= 1e-9:
            return float('inf')
            
        # å·¥ä½œé‡è®¡ç®—  
        work = self.w_0 + self.w_1 * total_tokens
        
        # æ€»å»¶è¿Ÿ
        latency = self.tau_0 + work / thr + self.tau_B * batch_size + self.tau_S * total_tokens
        
        return latency
    
    def _solve_optimal_tokens(self, batch_size: int, target_latency: float) -> Optional[int]:
        """ç»™å®šbatch_sizeæ±‚è§£æœ€ä¼˜total_tokens"""
        def latency_diff(S):
            return self._latency_function(batch_size, int(S)) - target_latency
        
        # æœç´¢åŒºé—´
        S_min = batch_size  # æœ€å°ï¼šå…¨decode
        S_max = min(self.config.max_num_scheduled_tokens, batch_size * 2048)
        
        # è¾¹ç•Œæ£€æŸ¥
        if latency_diff(S_min) > 0:
            if self.verbose:
                logger.warning(f"ç›®æ ‡å»¶è¿Ÿè¿‡ä½: {target_latency:.2f}ms")
            return None
            
        if latency_diff(S_max) < 0:
            if self.verbose:
                logger.warning(f"ç›®æ ‡å»¶è¿Ÿè¿‡é«˜: {target_latency:.2f}ms")
            return S_max
        
        # æ±‚è§£
        try:
            S_optimal = brentq(latency_diff, S_min, S_max, xtol=1e-3)
            return int(np.round(S_optimal))
        except ValueError as e:
            if self.verbose:
                logger.error(f"æ±‚è§£å¤±è´¥: {e}")
            return None
    
    def _schedule_running_requests(self, running_requests: List[RealRequestInfo], 
                                 available_tokens: int) -> Tuple[Dict[str, int], int]:
        """
        è°ƒåº¦runningé˜Ÿåˆ—è¯·æ±‚ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        
        ç­–ç•¥ï¼š
        1. decodeè¯·æ±‚ä¼˜å…ˆï¼Œæ¯ä¸ªåˆ†é…1ä¸ªtoken
        2. å†è°ƒåº¦chunked prefillï¼ŒæŒ‰å‰©ä½™éœ€æ±‚åˆ†é…
        
        Returns:
            (chunk_sizes, remaining_tokens)
        """
        chunk_sizes = {}
        remaining_tokens = available_tokens
        
        # åˆ†ç¦»decodeå’Œprefillè¯·æ±‚
        decode_reqs = [req for req in running_requests if req.is_decode_phase]
        prefill_reqs = [req for req in running_requests if not req.is_decode_phase]
        
        # 1. ä¼˜å…ˆè°ƒåº¦decodeè¯·æ±‚
        for req in decode_reqs:
            if remaining_tokens >= 1:
                chunk_sizes[req.request_id] = 1
                remaining_tokens -= 1
            else:
                chunk_sizes[req.request_id] = 0
        
        # 2. è°ƒåº¦running prefillè¯·æ±‚
        if prefill_reqs and remaining_tokens > 0:
            # æŒ‰ä¼˜å…ˆçº§å’Œå‰©ä½™éœ€æ±‚æ’åº
            prefill_reqs.sort(key=lambda r: (-r.priority, -r.remaining_prompt_tokens))
            
            for req in prefill_reqs:
                if remaining_tokens <= 0:
                    chunk_sizes[req.request_id] = 0
                    continue
                
                # è®¡ç®—å¯åˆ†é…çš„tokenæ•°
                max_needed = req.remaining_prompt_tokens
                max_chunk = min(req.max_chunk_size, self.config.long_prefill_threshold)
                allocation = min(max_needed, max_chunk, remaining_tokens)
                
                chunk_sizes[req.request_id] = allocation
                remaining_tokens -= allocation
        
        return chunk_sizes, remaining_tokens
    
    def _select_waiting_requests(self, waiting_requests: List[RealRequestInfo],
                               available_tokens: int, available_slots: int) -> Tuple[List[str], Dict[str, int], int]:
        """
        ä»waitingé˜Ÿåˆ—é€‰æ‹©è¯·æ±‚è¿›è¡Œè°ƒåº¦
        
        ç­–ç•¥ï¼š
        1. æŒ‰ä¼˜å…ˆçº§æ’åº
        2. è´ªå¿ƒé€‰æ‹©ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚è‡³å°‘èƒ½åˆ†é…æœ€å°chunk
        
        Returns:
            (selected_ids, chunk_sizes, remaining_tokens)
        """
        if not waiting_requests or available_tokens <= 0 or available_slots <= 0:
            return [], {}, available_tokens
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_waiting = sorted(waiting_requests, key=lambda r: -r.priority)
        
        selected_ids = []
        chunk_sizes = {}
        remaining_tokens = available_tokens
        remaining_slots = available_slots
        
        for req in sorted_waiting:
            if remaining_slots <= 0 or remaining_tokens <= 0:
                break
            
            # è®¡ç®—åˆå§‹åˆ†é…
            min_chunk = min(64, req.num_prompt_tokens)  # æœ€å°chunkå¤§å°
            max_chunk = min(req.max_chunk_size, self.config.long_prefill_threshold)
            
            if remaining_tokens < min_chunk:
                break  # å‰©ä½™tokenä¸è¶³ä»¥å¯åŠ¨æ–°è¯·æ±‚
            
            # åˆ†é…token
            allocation = min(req.num_prompt_tokens, max_chunk, remaining_tokens)
            
            selected_ids.append(req.request_id)
            chunk_sizes[req.request_id] = allocation
            remaining_tokens -= allocation
            remaining_slots -= 1
        
        return selected_ids, chunk_sizes, remaining_tokens
    
    def optimize_schedule(self, running_requests: List[RealRequestInfo],
                         waiting_requests: List[RealRequestInfo],
                         target_latency: float) -> OptimalSchedule:
        """
        ä¸»ä¼˜åŒ–ç®—æ³•ï¼šä¸ºç»™å®šç›®æ ‡å»¶è¿Ÿä¼˜åŒ–è°ƒåº¦å†³ç­–
        
        ç®—æ³•æµç¨‹ï¼š
        1. ä¼°ç®—åˆé€‚çš„batch_sizeå’Œtotal_tokens
        2. ä¼˜å…ˆè°ƒåº¦runningè¯·æ±‚
        3. è´ªå¿ƒé€‰æ‹©waitingè¯·æ±‚
        4. ç²¾ç»†è°ƒæ•´ä»¥é€¼è¿‘ç›®æ ‡å»¶è¿Ÿ
        """
        if self.verbose:
            logger.info(f"å¼€å§‹è°ƒåº¦ä¼˜åŒ–: running={len(running_requests)}, "
                       f"waiting={len(waiting_requests)}, target={target_latency:.2f}ms")
        
        # å½“å‰runningé˜Ÿåˆ—å¤§å°
        current_batch_size = len(running_requests)
        
        # å°è¯•ä¸åŒçš„batch_sizeé…ç½®
        best_schedule = None
        best_error = float('inf')
        
        # æœç´¢èŒƒå›´ï¼šå½“å‰batch_sizeåˆ°æœ€å¤§å¹¶å‘æ•°
        max_batch_size = min(
            current_batch_size + len(waiting_requests),
            self.config.max_num_running_reqs
        )
        
        for target_batch_size in range(current_batch_size, max_batch_size + 1):
            # æ±‚è§£è¯¥batch_sizeä¸‹çš„æœ€ä¼˜tokenæ•°
            optimal_tokens = self._solve_optimal_tokens(target_batch_size, target_latency)
            if optimal_tokens is None:
                continue
            
            # é™åˆ¶åœ¨é¢„ç®—å†…
            optimal_tokens = min(optimal_tokens, self.config.max_num_scheduled_tokens)
            
            # è°ƒåº¦runningè¯·æ±‚
            running_chunks, remaining_tokens = self._schedule_running_requests(
                running_requests, optimal_tokens
            )
            
            # è°ƒåº¦waitingè¯·æ±‚
            available_slots = target_batch_size - current_batch_size
            selected_waiting, waiting_chunks, final_remaining = self._select_waiting_requests(
                waiting_requests, remaining_tokens, available_slots
            )
            
            # è®¡ç®—å®é™…é…ç½®
            actual_batch_size = current_batch_size + len(selected_waiting)
            actual_tokens = optimal_tokens - final_remaining
            actual_latency = self._latency_function(actual_batch_size, actual_tokens)
            
            # è¯„ä¼°è¯¯å·®
            error = abs(actual_latency - target_latency)
            
            if error < best_error:
                best_error = error
                best_schedule = OptimalSchedule(
                    running_chunk_sizes=running_chunks,
                    scheduled_waiting_ids=selected_waiting,
                    waiting_chunk_sizes=waiting_chunks,
                    total_tokens=actual_tokens,
                    predicted_latency=actual_latency,
                    num_decode_tokens=sum(1 for req in running_requests if req.is_decode_phase),
                    num_prefill_tokens=actual_tokens - sum(1 for req in running_requests if req.is_decode_phase),
                    feasible=True
                )
        
        if best_schedule is None:
            # è¿”å›ä¿å®ˆæ–¹æ¡ˆ
            running_chunks, _ = self._schedule_running_requests(
                running_requests, self.config.max_num_scheduled_tokens
            )
            
            best_schedule = OptimalSchedule(
                running_chunk_sizes=running_chunks,
                scheduled_waiting_ids=[],
                waiting_chunk_sizes={},
                total_tokens=sum(running_chunks.values()),
                predicted_latency=self._latency_function(len(running_requests), sum(running_chunks.values())),
                num_decode_tokens=sum(1 for req in running_requests if req.is_decode_phase),
                num_prefill_tokens=sum(running_chunks.values()) - sum(1 for req in running_requests if req.is_decode_phase),
                feasible=False
            )
        
        if self.verbose:
            logger.info(f"ä¼˜åŒ–å®Œæˆ: batch_size={len(running_requests) + len(best_schedule.scheduled_waiting_ids)}, "
                       f"tokens={best_schedule.total_tokens}, "
                       f"latency={best_schedule.predicted_latency:.2f}ms, "
                       f"error={abs(best_schedule.predicted_latency - target_latency):.2f}ms")
        
        return best_schedule


def create_real_scenario_requests() -> Tuple[List[RealRequestInfo], List[RealRequestInfo]]:
    """åˆ›å»ºçœŸå®åœºæ™¯çš„è¯·æ±‚ç¤ºä¾‹"""
    
    # Runningè¯·æ±‚ï¼šæ··åˆdecodeå’Œchunked prefill
    running_requests = [
        # Decodeé˜¶æ®µçš„è¯·æ±‚
        RealRequestInfo("run_1", RequestPhase.DECODE, 512, 512, 256),
        RealRequestInfo("run_2", RequestPhase.DECODE, 256, 256, 128),
        RealRequestInfo("run_3", RequestPhase.DECODE, 1024, 1024, 512),
        
        # Chunked prefillé˜¶æ®µçš„è¯·æ±‚
        RealRequestInfo("run_4", RequestPhase.CHUNKED_PREFILL, 2048, 256, 128),
        RealRequestInfo("run_5", RequestPhase.CHUNKED_PREFILL, 1536, 512, 256),
    ]
    
    # Waitingè¯·æ±‚ï¼šéƒ½æ˜¯æ–°çš„prefillè¯·æ±‚
    waiting_requests = [
        RealRequestInfo("wait_1", RequestPhase.NEW_PREFILL, 1024, 0, 0, priority=2.0),
        RealRequestInfo("wait_2", RequestPhase.NEW_PREFILL, 512, 0, 0, priority=1.5),
        RealRequestInfo("wait_3", RequestPhase.NEW_PREFILL, 2048, 0, 0, priority=1.0),
        RealRequestInfo("wait_4", RequestPhase.NEW_PREFILL, 768, 0, 0, priority=3.0),
    ]
    
    return running_requests, waiting_requests


def demo_real_scheduler_optimization():
    """æ¼”ç¤ºçœŸå®è°ƒåº¦åœºæ™¯çš„ä¼˜åŒ–"""
    print("ğŸš€ vLLM çœŸå®è°ƒåº¦åœºæ™¯ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # æ¨¡å‹å‚æ•°
    model_params = {
        'P_max': 50.0,
        'k_B': 0.05,
        'k_S': 0.005,
        'w_0': 2.0,
        'w_1': 0.05,
        'tau_0': 5.0,
        'tau_B': 0.5,
        'tau_S': 0.01
    }
    
    # è°ƒåº¦é…ç½®
    config = SchedulingConfig(
        max_num_scheduled_tokens=4096,
        max_num_running_reqs=32,
        long_prefill_threshold=512
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = RealSchedulerOptimizer(model_params, config, verbose=True)
    
    # åˆ›å»ºæµ‹è¯•åœºæ™¯
    running_reqs, waiting_reqs = create_real_scenario_requests()
    
    print(f"\nğŸ“Š å½“å‰çŠ¶æ€:")
    print(f"Runningé˜Ÿåˆ—: {len(running_reqs)}ä¸ªè¯·æ±‚")
    for req in running_reqs:
        status = "Decode" if req.is_decode_phase else f"Prefill({req.remaining_prompt_tokens} left)"
        print(f"  - {req.request_id}: {status}")
    
    print(f"Waitingé˜Ÿåˆ—: {len(waiting_reqs)}ä¸ªè¯·æ±‚")
    for req in waiting_reqs:
        print(f"  - {req.request_id}: Prefill({req.num_prompt_tokens} tokens), Priority={req.priority}")
    
    # æµ‹è¯•ä¸åŒç›®æ ‡å»¶è¿Ÿ
    target_latencies = [30.0, 45.0, 60.0]
    
    for target_lat in target_latencies:
        print(f"\nğŸ¯ ç›®æ ‡å»¶è¿Ÿ: {target_lat}ms")
        print("-" * 40)
        
        schedule = optimizer.optimize_schedule(running_reqs, waiting_reqs, target_lat)
        
        if schedule.feasible:
            print(f"âœ… ä¼˜åŒ–æˆåŠŸ!")
            print(f"   é¢„æµ‹å»¶è¿Ÿ: {schedule.predicted_latency:.2f}ms")
            print(f"   è¯¯å·®: {abs(schedule.predicted_latency - target_lat):.2f}ms")
            print(f"   æ€»Tokenæ•°: {schedule.total_tokens}")
            print(f"   æ–°è°ƒåº¦è¯·æ±‚: {len(schedule.scheduled_waiting_ids)}ä¸ª")
            
            # è¯¦ç»†åˆ†é…ä¿¡æ¯
            print(f"   Runningè¯·æ±‚åˆ†é…:")
            for req_id, chunk_size in schedule.running_chunk_sizes.items():
                print(f"     {req_id}: {chunk_size} tokens")
            
            if schedule.scheduled_waiting_ids:
                print(f"   æ–°è°ƒåº¦çš„Waitingè¯·æ±‚:")
                for req_id in schedule.scheduled_waiting_ids:
                    chunk_size = schedule.waiting_chunk_sizes.get(req_id, 0)
                    print(f"     {req_id}: {chunk_size} tokens")
        else:
            print(f"âŒ æ— æ³•æ»¡è¶³ç›®æ ‡å»¶è¿Ÿï¼Œé‡‡ç”¨ä¿å®ˆæ–¹æ¡ˆ")
            print(f"   é¢„æµ‹å»¶è¿Ÿ: {schedule.predicted_latency:.2f}ms")


if __name__ == '__main__':
    demo_real_scheduler_optimization() 