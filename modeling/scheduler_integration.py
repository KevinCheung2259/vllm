#!/usr/bin/env python3
"""
vLLM Scheduler é›†æˆæ¨¡å—

å°†åå‘ä¼˜åŒ–ç®—æ³•ä¸ç°æœ‰æ€§èƒ½å»ºæ¨¡ç³»ç»Ÿé›†æˆï¼Œæä¾›å®Œæ•´çš„è°ƒåº¦å†³ç­–å·¥å…·ã€‚
"""

import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import logging

# å¯¼å…¥ç°æœ‰æ¨¡å—
from performance_model import ThroughputSaturationModel
from inverse_scheduler import InverseScheduler, RequestInfo, RequestStage, BatchConfig

logger = logging.getLogger(__name__)


class SchedulerOptimizer:
    """å®Œæ•´çš„è°ƒåº¦å™¨ä¼˜åŒ–å·¥å…·"""
    
    def __init__(self, fitted_model_path: Optional[str] = None, verbose: bool = True):
        """
        åˆå§‹åŒ–è°ƒåº¦ä¼˜åŒ–å™¨
        
        Args:
            fitted_model_path: å·²è®­ç»ƒæ¨¡å‹è·¯å¾„
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.verbose = verbose
        self.model = None
        self.inverse_scheduler = None
        
        if fitted_model_path and Path(fitted_model_path).exists():
            self.load_fitted_model(fitted_model_path)
        
    def load_fitted_model(self, model_path: str):
        """åŠ è½½å·²è®­ç»ƒçš„æ€§èƒ½æ¨¡å‹"""
        try:
            self.model = ThroughputSaturationModel(verbose=self.verbose)
            self.model.load_model(model_path)
            
            # æå–æ¨¡å‹å‚æ•°ç”¨äºåå‘è°ƒåº¦
            params = dict(zip(self.model.param_names, self.model.params))
            self.inverse_scheduler = InverseScheduler(params, verbose=self.verbose)
            
            if self.verbose:
                logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def fit_model_from_data(self, profiling_data_path: str):
        """ä»profilingæ•°æ®è®­ç»ƒæ–°æ¨¡å‹"""
        from integration import read_profiling_data
        
        # è¯»å–æ•°æ®
        df = read_profiling_data(profiling_data_path)
        if df is None or df.empty:
            raise ValueError("æ— æ³•è¯»å–æœ‰æ•ˆçš„profilingæ•°æ®")
        
        # è®­ç»ƒæ¨¡å‹
        self.model = ThroughputSaturationModel(verbose=self.verbose)
        self.model.fit(df)
        
        # åˆ›å»ºåå‘è°ƒåº¦å™¨
        params = dict(zip(self.model.param_names, self.model.params))
        self.inverse_scheduler = InverseScheduler(params, verbose=self.verbose)
        
        if self.verbose:
            logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def predict_latency(self, batch_size: int, total_tokens: int) -> float:
        """é¢„æµ‹ç»™å®šé…ç½®çš„å»¶è¿Ÿ"""
        if not self.model:
            raise ValueError("æ¨¡å‹å°šæœªåˆå§‹åŒ–")
        return self.model.predict(batch_size, total_tokens)
    
    def optimize_for_latency(self, requests: List[RequestInfo], 
                           target_latency: float) -> Optional[BatchConfig]:
        """ä¸ºç›®æ ‡å»¶è¿Ÿä¼˜åŒ–æ‰¹æ¬¡é…ç½®"""
        if not self.inverse_scheduler:
            raise ValueError("åå‘è°ƒåº¦å™¨å°šæœªåˆå§‹åŒ–")
        return self.inverse_scheduler.optimize_batch(requests, target_latency)
    
    def optimize_with_constraints(self, requests: List[RequestInfo],
                                target_latency: float,
                                max_chunk_size: int = 512,
                                min_prefill_ratio: float = 0.0) -> Optional[BatchConfig]:
        """å¸¦çº¦æŸçš„æ‰¹æ¬¡ä¼˜åŒ–"""
        if not self.inverse_scheduler:
            raise ValueError("åå‘è°ƒåº¦å™¨å°šæœªåˆå§‹åŒ–")
        return self.inverse_scheduler.optimize_with_constraints(
            requests, target_latency, max_chunk_size, min_prefill_ratio
        )
    
    def analyze_latency_sensitivity(self, base_requests: List[RequestInfo],
                                  latency_range: Tuple[float, float],
                                  n_points: int = 20) -> Dict:
        """åˆ†æå»¶è¿Ÿæ•æ„Ÿæ€§"""
        if not self.inverse_scheduler:
            raise ValueError("åå‘è°ƒåº¦å™¨å°šæœªåˆå§‹åŒ–")
        
        latencies = np.linspace(latency_range[0], latency_range[1], n_points)
        results = {
            'target_latencies': latencies,
            'achieved_latencies': [],
            'total_tokens': [],
            'decode_counts': [],
            'prefill_counts': [],
            'avg_prefill_sizes': [],
            'feasible': []
        }
        
        for target_lat in latencies:
            config = self.inverse_scheduler.optimize_batch(base_requests, target_lat)
            
            if config:
                results['achieved_latencies'].append(config.predicted_latency)
                results['total_tokens'].append(config.total_tokens)
                
                decode_count = sum(1 for cs in config.chunk_sizes if cs == 1)
                prefill_count = len(config.chunk_sizes) - decode_count
                avg_prefill = np.mean([cs for cs in config.chunk_sizes if cs > 1]) if prefill_count > 0 else 0
                
                results['decode_counts'].append(decode_count)
                results['prefill_counts'].append(prefill_count)
                results['avg_prefill_sizes'].append(avg_prefill)
                results['feasible'].append(True)
            else:
                # å¡«å……ç©ºå€¼
                results['achieved_latencies'].append(np.nan)
                results['total_tokens'].append(np.nan)
                results['decode_counts'].append(np.nan)
                results['prefill_counts'].append(np.nan)
                results['avg_prefill_sizes'].append(np.nan)
                results['feasible'].append(False)
        
        return results
    
    def plot_optimization_surface(self, request_counts: List[int],
                                latency_targets: List[float],
                                prefill_ratio: float = 0.3,
                                save_path: str = './modeling/optimization_surface.png'):
        """ç»˜åˆ¶ä¼˜åŒ–ç»“æœçš„3Dè¡¨é¢"""
        if not self.inverse_scheduler:
            raise ValueError("åå‘è°ƒåº¦å™¨å°šæœªåˆå§‹åŒ–")
        
        # åˆ›å»ºç½‘æ ¼
        R_mesh, L_mesh = np.meshgrid(request_counts, latency_targets)
        T_mesh = np.full_like(R_mesh, np.nan)
        
        # è®¡ç®—æ¯ä¸ªç‚¹çš„æœ€ä¼˜tokenæ•°
        for i, n_req in enumerate(request_counts):
            for j, target_lat in enumerate(latency_targets):
                # ç”Ÿæˆç¤ºä¾‹è¯·æ±‚
                requests = []
                for k in range(n_req):
                    if np.random.random() < prefill_ratio:
                        stage = RequestStage.PREFILL
                        remaining = np.random.randint(50, 300)
                    else:
                        stage = RequestStage.DECODE
                        remaining = 1
                    
                    requests.append(RequestInfo(
                        request_id=f"req_{k}",
                        stage=stage,
                        remaining_tokens=remaining
                    ))
                
                # ä¼˜åŒ–
                config = self.inverse_scheduler.optimize_batch(requests, target_lat)
                if config:
                    T_mesh[j, i] = config.total_tokens
        
        # ç»˜åˆ¶3Dè¡¨é¢
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®ç‚¹
        mask = ~np.isnan(T_mesh)
        if mask.any():
            surf = ax.plot_surface(R_mesh, L_mesh, T_mesh, 
                                 cmap='viridis', alpha=0.8,
                                 linewidth=0, antialiased=True)
            
            ax.set_xlabel('Number of Requests')
            ax.set_ylabel('Target Latency (ms)')
            ax.set_zlabel('Optimal Total Tokens')
            ax.set_title(f'Scheduler Optimization Surface\n(Prefill Ratio: {prefill_ratio*100:.0f}%)')
            
            fig.colorbar(surf, ax=ax, shrink=0.5, aspect=20)
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        if self.verbose:
            logger.info(f"ğŸ“Š Optimization surface plot saved: {save_path}")
        
        return fig
    
    def benchmark_scheduler_performance(self, test_scenarios: List[Dict]) -> pd.DataFrame:
        """åŸºå‡†æµ‹è¯•è°ƒåº¦å™¨æ€§èƒ½"""
        if not self.inverse_scheduler:
            raise ValueError("åå‘è°ƒåº¦å™¨å°šæœªåˆå§‹åŒ–")
        
        results = []
        
        for scenario in test_scenarios:
            n_requests = scenario['n_requests']
            target_latency = scenario['target_latency']
            prefill_ratio = scenario.get('prefill_ratio', 0.3)
            
            # ç”Ÿæˆæµ‹è¯•è¯·æ±‚
            requests = []
            for i in range(n_requests):
                if np.random.random() < prefill_ratio:
                    stage = RequestStage.PREFILL
                    remaining = np.random.randint(50, 500)
                else:
                    stage = RequestStage.DECODE
                    remaining = 1
                
                requests.append(RequestInfo(
                    request_id=f"req_{i}",
                    stage=stage,
                    remaining_tokens=remaining,
                    priority=scenario.get('priority', 1.0)
                ))
            
            # ä¼˜åŒ–
            import time
            start_time = time.time()
            config = self.inverse_scheduler.optimize_batch(requests, target_latency)
            optimization_time = time.time() - start_time
            
            # è®°å½•ç»“æœ
            if config:
                error = abs(config.predicted_latency - target_latency)
                relative_error = error / target_latency * 100
                
                results.append({
                    'n_requests': n_requests,
                    'target_latency': target_latency,
                    'prefill_ratio': prefill_ratio,
                    'achieved_latency': config.predicted_latency,
                    'total_tokens': config.total_tokens,
                    'error_ms': error,
                    'error_percent': relative_error,
                    'optimization_time_ms': optimization_time * 1000,
                    'feasible': True
                })
            else:
                results.append({
                    'n_requests': n_requests,
                    'target_latency': target_latency,
                    'prefill_ratio': prefill_ratio,
                    'achieved_latency': np.nan,
                    'total_tokens': np.nan,
                    'error_ms': np.nan,
                    'error_percent': np.nan,
                    'optimization_time_ms': optimization_time * 1000,
                    'feasible': False
                })
        
        return pd.DataFrame(results)
    
    def compare_scheduling_strategies(self, requests: List[RequestInfo],
                                    target_latency: float) -> Dict:
        """æ¯”è¾ƒä¸åŒè°ƒåº¦ç­–ç•¥"""
        if not self.model or not self.inverse_scheduler:
            raise ValueError("æ¨¡å‹å°šæœªåˆå§‹åŒ–")
        
        results = {}
        
        # 1. åå‘ä¼˜åŒ–ç­–ç•¥
        config_optimal = self.inverse_scheduler.optimize_batch(requests, target_latency)
        if config_optimal:
            results['optimal'] = {
                'total_tokens': config_optimal.total_tokens,
                'predicted_latency': config_optimal.predicted_latency,
                'chunk_sizes': config_optimal.chunk_sizes,
                'strategy': 'Inverse Optimization'
            }
        
        # 2. å‡åŒ€åˆ†é…ç­–ç•¥
        n_decode = sum(1 for r in requests if r.stage == RequestStage.DECODE)
        n_prefill = len(requests) - n_decode
        
        if config_optimal:
            # åŸºäºæœ€ä¼˜æ€»tokenæ•°å‡åŒ€åˆ†é…
            uniform_prefill_size = max(1, (config_optimal.total_tokens - n_decode) // max(n_prefill, 1))
            uniform_chunk_sizes = []
            for req in requests:
                if req.stage == RequestStage.DECODE:
                    uniform_chunk_sizes.append(1)
                else:
                    uniform_chunk_sizes.append(uniform_prefill_size)
            
            uniform_total = sum(uniform_chunk_sizes)
            uniform_latency = self.model.predict(len(requests), uniform_total)
            
            results['uniform'] = {
                'total_tokens': uniform_total,
                'predicted_latency': uniform_latency,
                'chunk_sizes': uniform_chunk_sizes,
                'strategy': 'Uniform Allocation'
            }
        
        # 3. è´ªå¿ƒç­–ç•¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        greedy_chunk_sizes = []
        for req in requests:
            if req.stage == RequestStage.DECODE:
                greedy_chunk_sizes.append(1)
            else:
                # ç®€å•è´ªå¿ƒï¼šåŸºäºå‰©ä½™tokenéœ€æ±‚
                greedy_chunk_sizes.append(min(req.remaining_tokens, 128))
        
        greedy_total = sum(greedy_chunk_sizes)
        greedy_latency = self.model.predict(len(requests), greedy_total)
        
        results['greedy'] = {
            'total_tokens': greedy_total,
            'predicted_latency': greedy_latency,
            'chunk_sizes': greedy_chunk_sizes,
            'strategy': 'Greedy Allocation'
        }
        
        return results


def demo_scheduler_integration():
    """æ¼”ç¤ºå®Œæ•´çš„è°ƒåº¦å™¨é›†æˆåŠŸèƒ½"""
    print("ğŸš€ vLLM è°ƒåº¦å™¨é›†æˆæ¼”ç¤º")
    print("=" * 50)
    
    # æ–¹å¼1ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒæ–°æ¨¡å‹
    print("\nğŸ“Š æ­¥éª¤1: è®­ç»ƒæ€§èƒ½æ¨¡å‹")
    print("-" * 30)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    np.random.seed(42)
    training_data = []
    for i in range(1000):
        B = np.random.randint(1, 33)
        if np.random.random() < 0.7:
            chunk_sizes = [1] * B  # decode-only
        else:
            chunk_sizes = []
            for _ in range(B):
                if np.random.random() < 0.3:
                    chunk_sizes.append(np.random.randint(10, 200))
                else:
                    chunk_sizes.append(1)
        
        S = sum(chunk_sizes)
        # ä½¿ç”¨çœŸå®æ¨¡å‹ç”Ÿæˆå»¶è¿Ÿ
        latency = 10 + 0.02 * S + 0.1 * B + np.random.normal(0, 2)
        latency = max(latency, 1.0)
        
        training_data.append({
            'chunk_sizes': chunk_sizes,
            'model_run_duration_ms': latency,
            'batch_id': i
        })
    
    df_train = pd.DataFrame(training_data)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å¹¶è®­ç»ƒ
    optimizer = SchedulerOptimizer(verbose=True)
    
    # æ‰‹åŠ¨è®­ç»ƒæ¨¡å‹ï¼ˆé¿å…æ–‡ä»¶ä¾èµ–ï¼‰
    optimizer.model = ThroughputSaturationModel(verbose=False)
    optimizer.model.fit(df_train)
    
    params = dict(zip(optimizer.model.param_names, optimizer.model.params))
    optimizer.inverse_scheduler = InverseScheduler(params, verbose=False)
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # æ­¥éª¤2ï¼šåå‘ä¼˜åŒ–æ¼”ç¤º
    print("\nğŸ¯ æ­¥éª¤2: åå‘è°ƒåº¦ä¼˜åŒ–")
    print("-" * 30)
    
    # åˆ›å»ºæµ‹è¯•è¯·æ±‚
    test_requests = [
        RequestInfo("req_0", RequestStage.DECODE),
        RequestInfo("req_1", RequestStage.DECODE),
        RequestInfo("req_2", RequestStage.PREFILL, remaining_tokens=100),
        RequestInfo("req_3", RequestStage.PREFILL, remaining_tokens=200),
        RequestInfo("req_4", RequestStage.DECODE),
    ]
    
    target_latency = 25.0
    config = optimizer.optimize_for_latency(test_requests, target_latency)
    
    if config:
        print(f"âœ… ä¼˜åŒ–æˆåŠŸ!")
        print(f"   ç›®æ ‡å»¶è¿Ÿ: {target_latency:.1f}ms")
        print(f"   å®é™…å»¶è¿Ÿ: {config.predicted_latency:.1f}ms")
        print(f"   è¯¯å·®: {abs(config.predicted_latency - target_latency):.1f}ms")
        print(f"   æ€»tokenæ•°: {config.total_tokens}")
        print(f"   chunkåˆ†é…: {config.chunk_sizes}")
    
    # æ­¥éª¤3ï¼šç­–ç•¥æ¯”è¾ƒ
    print("\nğŸ“ˆ æ­¥éª¤3: è°ƒåº¦ç­–ç•¥æ¯”è¾ƒ")
    print("-" * 30)
    
    strategies = optimizer.compare_scheduling_strategies(test_requests, target_latency)
    
    print(f"{'ç­–ç•¥':<20} {'å»¶è¿Ÿ(ms)':<12} {'Tokenæ•°':<10} {'è¯¯å·®(ms)':<10}")
    print("-" * 55)
    
    for name, result in strategies.items():
        error = abs(result['predicted_latency'] - target_latency)
        print(f"{result['strategy']:<20} {result['predicted_latency']:<12.1f} {result['total_tokens']:<10} {error:<10.1f}")
    
    # æ­¥éª¤4ï¼šåŸºå‡†æµ‹è¯•
    print("\nâš¡ æ­¥éª¤4: æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("-" * 30)
    
    test_scenarios = [
        {'n_requests': 8, 'target_latency': 20.0, 'prefill_ratio': 0.2},
        {'n_requests': 16, 'target_latency': 30.0, 'prefill_ratio': 0.3},
        {'n_requests': 32, 'target_latency': 50.0, 'prefill_ratio': 0.4},
    ]
    
    benchmark_results = optimizer.benchmark_scheduler_performance(test_scenarios)
    
    print("åŸºå‡†æµ‹è¯•ç»“æœ:")
    for _, row in benchmark_results.iterrows():
        if row['feasible']:
            print(f"  è¯·æ±‚æ•°={row['n_requests']:2d}, ç›®æ ‡={row['target_latency']:4.1f}ms, "
                  f"å®é™…={row['achieved_latency']:4.1f}ms, è¯¯å·®={row['error_percent']:4.1f}%, "
                  f"ä¼˜åŒ–æ—¶é—´={row['optimization_time_ms']:5.2f}ms")
        else:
            print(f"  è¯·æ±‚æ•°={row['n_requests']:2d}, ç›®æ ‡={row['target_latency']:4.1f}ms, ä¸å¯è¡Œ")
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")


if __name__ == '__main__':
    demo_scheduler_integration() 