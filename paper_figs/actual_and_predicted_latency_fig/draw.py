#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç”¨äºç”Ÿæˆ "Predicted vs Actual (Stable Model)" å›¾è¡¨
å®ç°ä»æ•°æ®è¯»å–ã€æ¨¡å‹è®­ç»ƒåˆ°å›¾è¡¨ç”Ÿæˆçš„ç«¯åˆ°ç«¯åŠŸèƒ½
"""

import sys
import os
import json
import numpy as np
import pandas as pd
import argparse
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Optional

# æ·»åŠ modelingç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'modeling'))

try:
    from performance_model import StableClusterModel
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä»ç»å¯¹è·¯å¾„å¯¼å…¥
    sys.path.insert(0, '/home/paperspace/zhangy/vllm-workspace/vllm/modeling')
    try:
        from performance_model import StableClusterModel
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥StableClusterModelï¼Œè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®")
        sys.exit(1)


class PredictedVsActualGenerator:

    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.models = []  # å­˜å‚¨å¤šä¸ªæ¨¡å‹
        self.dfs = []     # å­˜å‚¨å¤šä¸ªæ•°æ®é›†
        self.labels = []  # å­˜å‚¨æ¯ä¸ªæ•°æ®é›†çš„æ ‡ç­¾
        self.setup_style()

    def setup_style(self):
        """è®¾ç½®matplotlibæ ·å¼ï¼Œä¼˜åŒ–PDFæ¸²æŸ“æ€§èƒ½å¹¶ä¸å…¶ä»–å›¾é£æ ¼ä¸€è‡´"""
        plt.rcParams.update({
            'figure.dpi': 300,
            'savefig.dpi': 300,
            'savefig.bbox': 'tight',
            'savefig.pad_inches': 0.1,
            # é™ä½çŸ¢é‡è·¯å¾„å¤æ‚åº¦ï¼Œæå‡PDFæ¸²æŸ“é€Ÿåº¦
            'path.simplify': True,
            'path.simplify_threshold': 0.5,
            'agg.path.chunksize': 10000,
            'pdf.compression': 9,
        })
    
    def read_profiling_data(self, log_file_or_dir: str) -> pd.DataFrame:

        log_path = Path(log_file_or_dir)
        
        if log_path.is_dir():
            jsonl_files = list(log_path.glob('*.jsonl'))
            if not jsonl_files:
                print(f"âŒ åœ¨ç›®å½• {log_file_or_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°jsonlæ–‡ä»¶")
                return None
            log_files = jsonl_files
            if self.verbose:
                print(f"ğŸ“ æ‰¾åˆ°ç›®å½•: {log_file_or_dir}")
                print(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶: {len(jsonl_files)} ä¸ª")
        else:
            if not log_path.exists():
                print(f"âŒ æ—¥å¿—æ–‡ä»¶ {log_path} ä¸å­˜åœ¨")
                return None
            log_files = [log_path]
            if self.verbose:
                print(f"ğŸ“„ ä½¿ç”¨å•ä¸ªæ–‡ä»¶: {log_path}")
        
        # è¯»å–æ•°æ®
        data = []
        batch_id_offset = 0
        
        # æ’åºæ–‡ä»¶
        if len(log_files) > 1:
            try:
                log_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
            except Exception:
                log_files.sort(key=lambda x: x.name)
        
        for log_file in log_files:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # å»æ‰å‰å10è¡Œ
                for line in lines[10:-10]:
                    try:
                        entry = json.loads(line.strip())
                        if 'batch_id' in entry:
                            entry['batch_id'] += batch_id_offset
                        data.append(entry)
                    except json.JSONDecodeError:
                        continue
            batch_id_offset += len(lines[10:-10])
        
        if not data:
            return None
        
        # æ•°æ®æ¸…æ´—
        data = [item for item in data if item.get('schedule_duration_ms', 0) <200]
        data = [item for item in data if item.get('model_run_duration_ms', 0) < 200]
        
        df = pd.DataFrame(data)
        
        # è®¡ç®—decodeå’Œprefillè¯·æ±‚æ•°
        if 'chunk_sizes' in df.columns:
            def _count_decode_reqs(sizes):
                if isinstance(sizes, list):
                    return sum(1 for s in sizes if s == 1)
                try:
                    return 1 if sizes == 1 else 0
                except Exception:
                    return 0
            
            def _count_prefill_reqs(sizes):
                if isinstance(sizes, list):
                    return sum(1 for s in sizes if s > 1)
                try:
                    return 1 if sizes > 1 else 0
                except Exception:
                    return 0
            
            df['num_decode_reqs'] = df['chunk_sizes'].apply(_count_decode_reqs)
            df['num_prefill_reqs'] = df['chunk_sizes'].apply(_count_prefill_reqs)
        
        return df
    
    def train_model(self, df: pd.DataFrame) -> StableClusterModel:
        """è®­ç»ƒStableClusterModel"""

        if self.verbose:
            print("ğŸš€ å¼€å§‹è®­ç»ƒç¨³å®šé›†ç¾¤è°ƒåº¦æ¨¡å‹...")
        
        model = StableClusterModel(verbose=self.verbose)
        model.fit(df)
        
        if self.verbose:
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        return model
    
    def generate_multi_dataset_plot(self, models, dfs, labels=None,
                                   save_path: str = None, ranges:list[list] = None,
                                   rasterized: bool = False,
                                   max_points_per_dataset: Optional[int] = None,
                                   point_size: int = 50,
                                   alpha: float = 0.6,
                                   remove_edgecolors: bool = False) -> plt.Figure:
        """
        ç”ŸæˆåŒ…å«å¤šä¸ªæ•°æ®é›†çš„"Predicted vs Actual (Stable Model)"å›¾è¡¨
        
        Args:
            models: è®­ç»ƒå¥½çš„StableClusterModelå®ä¾‹åˆ—è¡¨
            dfs: åŒ…å«profilingæ•°æ®çš„DataFrameåˆ—è¡¨
            labels: æ•°æ®é›†æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            colors: æ•°æ®ç‚¹é¢œè‰²åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            save_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            matplotlib Figureå¯¹è±¡
        """
        # åˆ›å»ºå›¾è¡¨
        fig, ax = plt.subplots(figsize=(7, 6), dpi=300)
        
        # ç”¨äºç¡®å®šåæ ‡è½´èŒƒå›´
        all_T = []
        all_T_pred = []
        
        # é»˜è®¤é¢œè‰²å¾ªç¯
        default_colors = plt.cm.tab10.colors
        # é»˜è®¤æ ‡è®°æ ·å¼å¾ªç¯
        default_markers = ['o', 's', '^', 'v', 'D', '*', '+', 'x', 'p', 'h']
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†ç»˜åˆ¶æ•£ç‚¹å›¾
        for i, (model, df) in enumerate(zip(models, dfs)):
            # è·å–æ•°æ®
            B, S, T = model._preprocess_data(df)

            B_norm, S_norm, _ = model._normalize_features(B, S)
            
            # é¢„æµ‹å€¼
            T_pred = model.stable_latency_model((B_norm, S_norm), *model.params, model.P_max)
            
            # æ·»åŠ è¿‡æ»¤é€»è¾‘ï¼šåˆ é™¤Tä¸­æ•°å€¼å¤§äº100çš„æ•°æ®ç‚¹ï¼ŒåŒæ—¶åˆ é™¤å¯¹åº”çš„T_pred
            # å°†Tå’ŒT_predè½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿è¿›è¡Œå‘é‡åŒ–æ“ä½œ
            import numpy as np
            T = np.array(T)
            T_pred = np.array(T_pred)
            
            # åº”ç”¨ mask_1 è¿‡æ»¤æ•°æ®
            mask_1 = (T>ranges[i][0]) & (T<ranges[i][1])
            T_1 = T[mask_1]
            T_pred_1 = T_pred[mask_1]

            mask_2 = (T_pred_1>ranges[i][0]) & (T_pred_1<ranges[i][1])
            T_2 = T_1[mask_2]
            T_pred_2 = T_pred_1[mask_2]
            
            # ä¸‹é‡‡æ ·ï¼šæŒ‰æ•°æ®é›†é™åˆ¶æœ€å¤§ç‚¹æ•°
            if max_points_per_dataset is not None and len(T_2) > max_points_per_dataset:
                rng = np.random.default_rng(42)
                idx = rng.choice(len(T_2), size=max_points_per_dataset, replace=False)
                T_2 = T_2[idx]
                T_pred_2 = T_pred_2[idx]

            # å¦‚æœè¯¥æ•°æ®é›†åœ¨è¿‡æ»¤/ä¸‹é‡‡æ ·åä¸ºç©ºï¼Œåˆ™è·³è¿‡
            if len(T_2) == 0:
                continue

            # å­˜å‚¨è¿‡æ»¤åçš„å€¼ä»¥ç¡®å®šèŒƒå›´
            all_T.extend(T_2)
            all_T_pred.extend(T_pred_2)


            # # å­˜å‚¨æ‰€æœ‰å€¼ä»¥ç¡®å®šèŒƒå›´
            # all_T.extend(T)
            # all_T_pred.extend(T_pred)
            
            # è·å–æ ‡ç­¾ã€é¢œè‰²å’Œæ ‡è®°
            label = labels[i]
            color = default_colors[i % len(default_colors)]
            marker = default_markers[i % len(default_markers)]
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾ï¼ˆå¯é€‰ï¼šæ …æ ¼åŒ–ã€å»è¾¹æ¡†ï¼‰
            edgecolor_value = 'none' if remove_edgecolors else 'white'
            linewidth_value = 0.0 if remove_edgecolors else 0.5
            ax.scatter(
                T_pred_2,
                T_2,
                alpha=alpha,
                s=point_size,
                c=[color],
                marker=marker,
                label=label,
                edgecolors=edgecolor_value,
                linewidth=linewidth_value,
                rasterized=rasterized,
            )
        
        # ç†æƒ³çº¿ (y=x)
        min_val = min(min(all_T), min(all_T_pred))
        max_val = max(max(all_T), max(all_T_pred))
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=4, label='Ideal (y=x)')
        
        # è®¾ç½®å›¾è¡¨å±æ€§
        ax.set_xlabel('Predicted Latency (ms)', fontsize=23,labelpad=10)
        ax.set_ylabel('Actual Latency (ms)', fontsize=23,labelpad=10)
        # ax.set_title('Predicted vs Actual latency', fontsize=18,pad=20)
        ax.grid(True, linestyle='--', alpha=0.3, linewidth=2)
        
        # æ·»åŠ å›¾ä¾‹
        ax.legend(loc='upper left',fontsize = 18,markerscale=1.8)
        # è®¾ç½®åæ ‡è½´åˆ»åº¦å­—ä½“å¤§å°
        ax.tick_params(axis='both', pad=8, labelsize=19)  
        # æ§åˆ¶ç»˜å›¾åŒºåŸŸä¸å›¾ç‰‡ä¸Šä¸‹å·¦å³çš„é—´è·
        plt.subplots_adjust(left=0.15, right=0.8, bottom=0.15, top=0.8)

        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨ï¼ˆå¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼‰
        if save_path:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            if self.verbose:
                print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
        
            pdf_path = save_path.replace(".png",".pdf")
            plt.savefig(pdf_path, dpi=300, bbox_inches='tight', format='pdf')

    
        return fig
    
    def run_multi_dataset_end_to_end(self, log_paths: list, save_path: str = None, 
                                     labels: list = None, ranges:list[list] = None,
                                     rasterized: bool = False,
                                     max_points_per_dataset: Optional[int] = None,
                                     point_size: int = 50,
                                     alpha: float = 0.6,
                                     remove_edgecolors: bool = False) -> bool:
        """
        è¿è¡Œç«¯åˆ°ç«¯çš„æµç¨‹å¤„ç†å¤šä¸ªæ•°æ®é›†ï¼šè¯»å–æ•°æ®ã€è®­ç»ƒæ¨¡å‹ã€ç”Ÿæˆåˆå¹¶å›¾è¡¨
        
        Args:
            log_paths: æ—¥å¿—æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„åˆ—è¡¨
            save_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            labels: æ•°æ®é›†æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸæ‰§è¡Œ
        """
        try:
            models = []
            dfs = []
            
            # ä¸ºæ¯ä¸ªæ•°æ®é›†è¯»å–æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹
            for i, log_path in enumerate(log_paths):
                # 1.è¯»å–æ•°æ®
                if self.verbose:
                    print(f"\nğŸ“¥ æ­£åœ¨è¯»å–æ•°æ®é›† {i+1}/{len(log_paths)} çš„profilingæ•°æ®...")

                # è·å–å½“å‰æ•°æ®é›†çš„æ¸…æ´—é˜ˆå€¼ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
                
                df = self.read_profiling_data(log_path)
                if df is None or df.empty:
                    print(f"âŒ æ•°æ®é›† {i+1} æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                    continue
                dfs.append(df)
                
                if self.verbose:
                    print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡profilingæ•°æ®")
                
                # 2.è®­ç»ƒæ¨¡å‹
                model = self.train_model(df)
                models.append(model)
            
            if not models or not dfs:
                print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†")
                return False
            
            # ä¿å­˜æ¨¡å‹å’Œæ•°æ®ä»¥ä¾¿åç»­ä½¿ç”¨
            self.models = models
            self.dfs = dfs
            if labels:
                self.labels = labels
            
            # 3.ç”Ÿæˆå¤šæ•°æ®é›†å›¾è¡¨
            if self.verbose:
                print("\nğŸ“Š æ­£åœ¨ç”ŸæˆåŒ…å«å¤šä¸ªæ•°æ®é›†çš„'Predicted vs Actual'å›¾è¡¨...")
            self.generate_multi_dataset_plot(
                models,
                dfs,
                labels,
                save_path=save_path,
                ranges=ranges,
                rasterized=rasterized,
                max_points_per_dataset=max_points_per_dataset,
                point_size=point_size,
                alpha=alpha,
                remove_edgecolors=remove_edgecolors,
            )
            
            return True
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


def main():

    parser = argparse.ArgumentParser(description='ç”ŸæˆPredicted vs Actual (Stable Model)å›¾è¡¨')
    parser.add_argument('log_path', type=str, nargs='*',
                      help='profilingæ•°æ®æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ (å¯æŒ‡å®šå¤šä¸ªï¼Œé»˜è®¤: profiling_result)')
    parser.add_argument('--save-path', type=str, default="./predicted_and_actual_latency.pdf")
    parser.add_argument('--labels', type=str, nargs='*',help='ä¸ºæ¯ä¸ªæ•°æ®é›†æŒ‡å®šæ ‡ç­¾ (ä¸log_pathé¡ºåºå¯¹åº”)')
    parser.add_argument('--rasterized', action='store_true', help='å°†æ•£ç‚¹ä»¥æ …æ ¼æ–¹å¼åµŒå…¥PDFï¼Œæ˜¾è‘—é™ä½PDFæ¸²æŸ“å¼€é”€')
    parser.add_argument('--max-points-per-dataset', type=int, default=None, help='æ¯ä¸ªæ•°æ®é›†æœ€å¤šç»˜åˆ¶çš„ç‚¹æ•°ï¼Œè¶…å‡ºå°†éšæœºä¸‹é‡‡æ ·')
    parser.add_argument('--point-size', type=int, default=50, help='æ•£ç‚¹å¤§å°')
    parser.add_argument('--alpha', type=float, default=0.6, help='æ•£ç‚¹é€æ˜åº¦')
    parser.add_argument('--no-edges', action='store_true', help='ç§»é™¤æ•£ç‚¹è¾¹æ¡†ä»¥å‡å°‘çŸ¢é‡è·¯å¾„å¤æ‚åº¦')
    
    args = parser.parse_args()
    
    generator = PredictedVsActualGenerator()
    base_dir = "/home/paperspace/zhangy/vllm-workspace/vllm/exp"
    default_data = {
        "H100":{"log_path":f"{base_dir}/profiling_result_h100","T_range":[0,200]},
        "A100":{"log_path":f"{base_dir}/profiling_result_a100","T_range":[100,200]},
        "A6000-TP2":{"log_path":f"{base_dir}/profiling_result_a6000","T_range":[100,200]},
        "H100-32B":{"log_path":f"{base_dir}/profiling_result_h100_qwen32b","T_range":[25,200]},
    }

    labels = list(default_data.keys())  # è·å–æ‰€æœ‰æ ‡ç­¾
    log_paths = [default_data[label]["log_path"] for label in default_data]
    ranges = [default_data[label]["T_range"] for label in default_data]

    # å¤šä¸ªæ•°æ®é›†æ¨¡å¼
    success = generator.run_multi_dataset_end_to_end(
        log_paths=args.log_path if args.log_path else log_paths,
        save_path=args.save_path,
        labels=args.labels if args.labels else labels,
        ranges = ranges,
        rasterized=args.rasterized,
        max_points_per_dataset=args.max_points_per_dataset,
        point_size=args.point_size,
        alpha=args.alpha,
        remove_edgecolors=args.no_edges,
    )
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()