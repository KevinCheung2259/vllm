#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batch Size vs Total Tokens çƒ­åŠ›å›¾ç»˜åˆ¶è„šæœ¬
å‚è€ƒdraw_chunk_runtimeæ ·å¼ï¼Œç”¨äºè®ºæ–‡æŠ•ç¨¿

ç»˜åˆ¶æ‰¹æ¬¡å¤§å°(Batch Size)ä¸æ€»tokenæ•°(Total Tokens)å¯¹æ¨¡å‹è¿è¡Œæ—¶é—´çš„å½±å“
æ”¯æŒæ•°æ®ç¼ºå¤±æ—¶çš„æ¨¡å‹æ‹Ÿåˆå¡«å……

ä½¿ç”¨æ–¹æ³•:
python draw_heatmap.py <æ•°æ®æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„>
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from pathlib import Path
from scipy.optimize import minimize
from scipy.interpolate import griddata
from scipy.ndimage import gaussian_filter
import argparse
import traceback

class BatchTokenHeatmapGenerator:
    """ç”ŸæˆBatch Size vs Total Tokensçƒ­åŠ›å›¾çš„ç±»"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        # å‚è€ƒdraw_chunk_runtimeçš„æ ·å¼è®¾ç½®
        self.setup_style()
    
    def setup_style(self):
        """è®¾ç½®matplotlibæ ·å¼ï¼Œå‚è€ƒdraw_chunk_runtime"""
        plt.rcParams.update({
            'font.size': 12,
            'axes.labelsize': 23,  # å‚è€ƒdraw_chunk_runtime
            'xtick.labelsize': 19, # å‚è€ƒdraw_chunk_runtime
            'ytick.labelsize': 19, # å‚è€ƒdraw_chunk_runtime
            'legend.fontsize': 16,
            'figure.figsize': (7, 6),  # å‚è€ƒdraw_chunk_runtime
            'figure.dpi': 300,
            'savefig.dpi': 300,
            'savefig.bbox': 'tight',
            'savefig.pad_inches': 0.1,
            'axes.unicode_minus': False,
        })
    
    def read_profiling_data(self, log_file_or_dir: str) -> pd.DataFrame:
        """è¯»å–profilingæ•°æ®ï¼Œå‚è€ƒdraw_chunk_runtimeçš„æ•°æ®å¤„ç†æ–¹å¼"""
        log_path = Path(log_file_or_dir)
        
        if log_path.is_dir():
            jsonl_files = list(log_path.glob('*.jsonl'))
            if not jsonl_files:
                if self.verbose:
                    print(f"âŒ åœ¨ç›®å½• {log_file_or_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°jsonlæ–‡ä»¶")
                return None
            log_files = jsonl_files
            if self.verbose:
                print(f"ğŸ“ æ‰¾åˆ°ç›®å½•: {log_file_or_dir}")
                print(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶: {len(jsonl_files)} ä¸ª")
        else:
            if not log_path.exists():
                if self.verbose:
                    print(f"âŒ æ—¥å¿—æ–‡ä»¶ {log_path} ä¸å­˜åœ¨")
                return None
            log_files = [log_path]
            if self.verbose:
                print(f"ğŸ“„ ä½¿ç”¨å•ä¸ªæ–‡ä»¶: {log_path}")
        
        # è¯»å–æ•°æ®
        data = []
        batch_id_offset = 0
        
        # æ’åºæ–‡ä»¶
        if len(log_files) > 1:
            try:
                log_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
            except Exception:
                log_files.sort(key=lambda x: x.name)
        
        for log_file in log_files:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # è·³è¿‡å‰å10è¡Œä»¥é¿å…ä¸ç¨³å®šæ•°æ®
                for line in lines[10:-10]:
                    try:
                        entry = json.loads(line.strip())
                        if 'batch_id' in entry:
                            entry['batch_id'] += batch_id_offset
                        data.append(entry)
                    except json.JSONDecodeError:
                        continue
            batch_id_offset += len(lines[10:-10])
        
        if not data:
            if self.verbose:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„profilingæ•°æ®")
            return None
        
        df = pd.DataFrame(data)
        
        # æ•°æ®æ¸…æ´—ï¼šç§»é™¤å¼‚å¸¸å€¼
        if self.verbose:
            print(f"ğŸ“Š åŸå§‹æ•°æ®ç‚¹æ•°: {len(df)}")
        
        df = df[df.get('schedule_duration_ms', 0) < 300]
        df = df[df.get('model_run_duration_ms', 0) < 200]
        
        if self.verbose:
            print(f"âœ… æ¸…æ´—åæ•°æ®ç‚¹æ•°: {len(df)}")
            print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡profilingæ•°æ®")
        
        return df
    
    def extract_batch_and_token_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–batch sizeå’Œtotal tokensç‰¹å¾"""
        def get_batch_size(sizes):
            if isinstance(sizes, list):
                return len(sizes)
            return np.nan
        
        def get_total_tokens(sizes):
            if isinstance(sizes, list):
                return sum(sizes)
            if isinstance(sizes, (int, float)):
                return float(sizes)
            return np.nan
        
        df['batch_size'] = df['chunk_sizes'].apply(get_batch_size)
        df['total_tokens'] = df['chunk_sizes'].apply(get_total_tokens)
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_df = df[
            (df['batch_size'] >= 1) & (df['batch_size'] <= 120) &
            (df['total_tokens'] >= 1) & (df['total_tokens'] <= 4096) &
            (df['model_run_duration_ms'].notna())
        ].copy()
        
        if self.verbose:
            print(f"ğŸ“ˆ æå–ç‰¹å¾åæœ‰æ•ˆæ•°æ®ç‚¹æ•°: {len(valid_df)}")
            print(f"ğŸ“Š Batch Size èŒƒå›´: {valid_df['batch_size'].min():.0f} - {valid_df['batch_size'].max():.0f}")
            print(f"ğŸ“Š Total Tokens èŒƒå›´: {valid_df['total_tokens'].min():.0f} - {valid_df['total_tokens'].max():.0f}")
            print(f"ğŸ“Š Model Runtime èŒƒå›´: {valid_df['model_run_duration_ms'].min():.2f} - {valid_df['model_run_duration_ms'].max():.2f} ms")
        
        return valid_df
    
    def runtime_model_function(self, batch_size, total_tokens, params):
        """
        è¿è¡Œæ—¶é—´æ¨¡å‹å‡½æ•°ï¼šT(B,S) = T0 + Î±S + Î³â‹…(1-e^(-B/B0))
        - T0: åŸºç¡€æ—¶é—´
        - Î±S: å¯¹tokenæ•°Sçš„çº¿æ€§å¢é•¿
        - Î³â‹…(1-e^(-B/B0)): å¯¹batch size Bçš„é¥±å’Œå¢é•¿å‡½æ•°
        """
        # åŸºç¡€æ—¶é—´
        T0 = params['base_time']
        
        # çº¿æ€§å¢é•¿é¡¹ (total_tokens)
        linear_term = params['alpha'] * total_tokens
        
        # é¥±å’Œå¢é•¿é¡¹ (batch_size) - å…ˆå¿«é€Ÿå¢é•¿åè¶‹å¹³
        saturation_term = params['gamma'] * (1 - np.exp(-batch_size / params['B0']))
        
        return T0 + linear_term + saturation_term
    
    def fit_model_to_data(self, valid_df: pd.DataFrame):
        """æ‹Ÿåˆæ¨¡å‹å‚æ•°åˆ°å®é™…æ•°æ®"""
        
        # æ‹Ÿåˆå‰å…ˆå‰”é™¤æ‰[batchsizeåœ¨24-96ä¸”tokenæ•°åœ¨1000-3840]çš„æ•°æ®
        # ä½¿ç”¨NOTæ¡ä»¶æ¥å‰”é™¤åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶çš„æ•°æ®
        filtered_df = valid_df
        
        # ä»è¿‡æ»¤åçš„æ•°æ®è·å–æ•°ç»„
        batch_sizes = filtered_df['batch_size'].values
        total_tokens = filtered_df['total_tokens'].values
        runtimes = filtered_df['model_run_duration_ms'].values
        
        if self.verbose:
            print(f"ğŸ“Š æ‹Ÿåˆæ•°æ®ç‚¹æ•°: {len(filtered_df)} (åŸå§‹: {len(valid_df)})")
        
        def objective(params_vec):
            params = {
                'base_time': params_vec[0],    # T0
                'alpha': params_vec[1],        # Î± (çº¿æ€§ç³»æ•°)
                'gamma': params_vec[2],        # Î³ (é¥±å’Œæœ€å¤§å€¼)
                'B0': params_vec[3]            # B0 (é¥±å’Œå‚æ•°)
            }
            predicted = self.runtime_model_function(batch_sizes, total_tokens, params)
            return np.mean((predicted - runtimes) ** 2)
        
        # è°ƒæ•´åˆå§‹å‚æ•°å’Œè¾¹ç•Œä»¥é€‚åº”æ–°çš„é¥±å’Œæ¨¡å‹
        # [base_time(T0), alpha(Î±), gamma(Î³), B0]
        initial_guess = [10.0, 0.01, 30.0, 20.0]
        bounds = [(1, 50), (0.001, 0.1), (5, 100), (5, 100)]
        
        result = minimize(objective, initial_guess, bounds=bounds, method='L-BFGS-B')
        
        fitted_params = {
            'base_time': result.x[0],  # T0
            'alpha': result.x[1],      # Î±
            'gamma': result.x[2],      # Î³
            'B0': result.x[3]          # B0
        }
        
        # è®¡ç®—RÂ²
        predicted = self.runtime_model_function(batch_sizes, total_tokens, fitted_params)
        ss_res = np.sum((runtimes - predicted) ** 2)
        ss_tot = np.sum((runtimes - np.mean(runtimes)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        if self.verbose:
            print(f"ğŸ“ æ¨¡å‹æ‹Ÿåˆå®Œæˆ:")
            print(f"   åŸºç¡€æ—¶é—´ (T0): {fitted_params['base_time']:.2f} ms")
            print(f"   çº¿æ€§ç³»æ•° (Î±): {fitted_params['alpha']:.6f}")
            print(f"   é¥±å’Œæœ€å¤§å€¼ (Î³): {fitted_params['gamma']:.2f}")
            print(f"   é¥±å’Œå‚æ•° (B0): {fitted_params['B0']:.2f}")
            print(f"   RÂ² = {r_squared:.4f}")
            
            # éªŒè¯æ¨¡å‹è¡Œä¸º - é¥±å’Œå‡½æ•°åº”è¯¥è¡¨ç°ä¸ºbatch sizeå¢é•¿æ—¶è¾¹é™…æ•ˆåº”é€’å‡
            test_batch_sizes = [1, 12, 30, 60, 100]
            test_tokens = [100, 1000, 3500]
            print(f"ğŸ“Š æ¨¡å‹éªŒè¯ (é¥±å’Œå‡½æ•°éªŒè¯) - æ³¨æ„ï¼šä»¥ä¸‹ä¸ºåŸå§‹é¢„æµ‹å€¼ï¼Œçƒ­åŠ›å›¾ä¸­ä¼šé™åˆ¶å¹¶å¹³æ»‘å¤„ç†:")
            for tokens in test_tokens:
                print(f"   Token={tokens}:")
                prev_runtime = 0
                for batch_size in test_batch_sizes:
                    predicted = self.runtime_model_function(batch_size, tokens, fitted_params)
                    if prev_runtime > 0:
                        increment = predicted - prev_runtime
                        print(f"     Batch={batch_size}: {predicted:.2f}ms (+{increment:.2f})")
                    else:
                        print(f"     Batch={batch_size}: {predicted:.2f}ms")
                    prev_runtime = predicted
        
        return fitted_params, r_squared
    
    def create_heatmap_data(self, valid_df: pd.DataFrame, resolution=(25, 25)):
        """åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®ï¼Œæ™ºèƒ½å¤„ç†ç¼ºå¤±æ•°æ®"""
        
        # åŸºäºå®é™…æ•°æ®ç¡®å®šèŒƒå›´
        batch_min = max(1, valid_df['batch_size'].min())
        batch_max = min(120, valid_df['batch_size'].max())
        token_min = max(1, valid_df['total_tokens'].min())
        token_max = min(4096, valid_df['total_tokens'].quantile(0.99))
        
        if self.verbose:
            print(f'ğŸ“Š æ•°æ®èŒƒå›´: batch_size [{batch_min:.0f}, {batch_max:.0f}], total_tokens [{token_min:.0f}, {token_max:.0f}]')
        
        # æ‹Ÿåˆæ¨¡å‹åˆ°ç°æœ‰æ•°æ®
        fitted_params, r_squared = self.fit_model_to_data(valid_df)
        
        # åˆ›å»ºç½‘æ ¼
        batch_bins, token_bins = resolution
        batch_grid = np.linspace(batch_min, batch_max, batch_bins)
        token_grid = np.linspace(token_min, token_max, token_bins)
        
        # åˆå§‹åŒ–è¿è¡Œæ—¶é—´çŸ©é˜µ
        runtime_matrix = np.full((token_bins, batch_bins), np.nan)
        
        # å°†å®é™…æ•°æ®æ˜ å°„åˆ°ç½‘æ ¼ - åˆ›å»ºæ•°æ®å¯†åº¦æ›´é«˜çš„åŒºåŸŸ
        for _, row in valid_df.iterrows():
            batch_idx = np.argmin(np.abs(batch_grid - row['batch_size']))
            token_idx = np.argmin(np.abs(token_grid - row['total_tokens']))
            
            if np.isnan(runtime_matrix[token_idx, batch_idx]):
                runtime_matrix[token_idx, batch_idx] = row['model_run_duration_ms']
            else:
                # å¦‚æœå·²æœ‰æ•°æ®ï¼Œç”¨åŸæ¥çš„å€¼ä¸æ¨¡å‹é¢„æµ‹å€¼å¹³å‡
                runtime_matrix[token_idx, batch_idx] = (
                    runtime_matrix[token_idx, batch_idx] + row['model_run_duration_ms']) / 2
        
        # ç»Ÿè®¡æ•°æ®è¦†ç›–ç‡
        data_coverage = np.sum(~np.isnan(runtime_matrix)) / runtime_matrix.size
        if self.verbose:
            print(f'ğŸ“Š æ•°æ®è¦†ç›–ç‡: {data_coverage:.1%}')
        
        # ä½¿ç”¨æ¨¡å‹å¡«å……ç¼ºå¤±æ•°æ®ï¼ˆä½¿ç”¨åŸå§‹é¢„æµ‹å€¼ï¼‰
        batch_mesh, token_mesh = np.meshgrid(batch_grid, token_grid)
        for i in range(token_bins):
            for j in range(batch_bins):
                if np.isnan(runtime_matrix[i, j]):
                    predicted_runtime = self.runtime_model_function(
                        batch_mesh[i, j], token_mesh[i, j], fitted_params
                    )
                    # ç›´æ¥ä½¿ç”¨åŸå§‹é¢„æµ‹å€¼ï¼Œä¸è¿›è¡Œé™åˆ¶å¤„ç†
                    runtime_matrix[i, j] = predicted_runtime
        
        # ä¸è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œä¿æŒåŸå§‹é¢„æµ‹å€¼
        # runtime_matrix = gaussian_filter(runtime_matrix, sigma=0.8)
        
        if self.verbose:
            print(f"ğŸ“Š çƒ­åŠ›å›¾ç½‘æ ¼å¤§å°: {runtime_matrix.shape}")
            print(f"ğŸ“Š è¿è¡Œæ—¶é—´èŒƒå›´ (åŸå§‹æ¨¡å‹é¢„æµ‹å€¼): {runtime_matrix.min():.2f} - {runtime_matrix.max():.2f} ms")
        
        return runtime_matrix, batch_grid, token_grid, fitted_params, r_squared
    
    def plot_heatmap(self, runtime_matrix, batch_grid, token_grid, fitted_params, r_squared, 
                    save_path: str = None) -> plt.Figure:
        """ç»˜åˆ¶çƒ­åŠ›å›¾ï¼Œå‚è€ƒdraw_chunk_runtimeæ ·å¼"""
        
        # åˆ›å»ºå›¾è¡¨ï¼Œå‚è€ƒdraw_chunk_runtimeçš„å°ºå¯¸
        fig, ax = plt.subplots(figsize=(7, 6), dpi=300)
        
        # ä½¿ç”¨viridisé¢œè‰²æ˜ å°„
        cmap = plt.cm.viridis
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾ï¼Œå…³é”®ï¼šä¿®æ”¹extentå‚æ•°ç¡®ä¿åæ ‡è½´æ˜¾ç¤ºå®é™…æ•°å€¼è€ŒéèŒƒå›´
        im = ax.imshow(
            runtime_matrix,
            extent=[batch_grid[0], batch_grid[-1], token_grid[0], token_grid[-1]],
            origin='lower',
            aspect='auto',
            cmap=cmap,
            interpolation='bilinear'
        )
        
        # åŸºäºæ‹Ÿåˆæ¨¡å‹ç”Ÿæˆç†è®ºç­‰é«˜çº¿
        # 1. åˆ›å»ºé«˜åˆ†è¾¨ç‡ç½‘æ ¼ç”¨äºç†è®ºæ¨¡å‹é¢„æµ‹
        model_resolution = (100, 100)  # é«˜åˆ†è¾¨ç‡ç¡®ä¿å¹³æ»‘
        batch_model = np.linspace(batch_grid[0], batch_grid[-1], model_resolution[1])
        token_model = np.linspace(token_grid[0], token_grid[-1], model_resolution[0])
        batch_mesh_model, token_mesh_model = np.meshgrid(batch_model, token_model)
        
        # 2. ä½¿ç”¨æ‹Ÿåˆæ¨¡å‹ç”Ÿæˆç†è®ºé¢„æµ‹å€¼
        runtime_theory = self.runtime_model_function(
            batch_mesh_model, token_mesh_model, fitted_params
        )
        
        # 3. è®¾ç½®ç†è®ºç­‰é«˜çº¿çº§åˆ«
        theory_min = np.min(runtime_theory)
        theory_max = np.max(runtime_theory)
        # ä½¿ç”¨åˆç†çš„ç­‰é«˜çº¿é—´éš”
        contour_levels = np.linspace(theory_min, theory_max, 10)
        
        # 4. ç»˜åˆ¶åŸºäºç†è®ºæ¨¡å‹çš„å¹³æ»‘ç­‰é«˜çº¿
        contours = ax.contour(
            batch_mesh_model, token_mesh_model, runtime_theory,
            levels=contour_levels,
            colors='white',
            linewidths=1.2,
            alpha=0.8,
            linestyles='-'
        )
        
        # 5. æ·»åŠ ç­‰é«˜çº¿æ ‡ç­¾
        ax.clabel(contours, inline=True, fontsize=14, fmt='%.0f', colors='white')
        
        # è®¾ç½®åæ ‡è½´æ ‡ç­¾ï¼Œå‚è€ƒdraw_chunk_runtimeçš„æ ·å¼
        ax.set_xlabel('Batch Size', fontsize=23, labelpad=10)  # å‚è€ƒdraw_chunk_runtime
        ax.set_ylabel('Total Scheduled Tokens', fontsize=23, labelpad=10)  # å‚è€ƒdraw_chunk_runtime
        
        # è®¾ç½®åæ ‡è½´åˆ»åº¦ - ç¡®ä¿æ˜¾ç¤ºå…·ä½“æ•°å€¼è€ŒéèŒƒå›´
        # Batch Sizeåˆ»åº¦
        batch_ticks = np.linspace(batch_grid[0], batch_grid[-1], 6).astype(int)
        ax.set_xticks(batch_ticks)
        ax.set_xticklabels(batch_ticks)
        
        # Total Tokensåˆ»åº¦ - ä½¿ç”¨å›ºå®šåˆ»åº¦å€¼
        token_ticks = [1, 1024, 2048, 3072, 4096]
        ax.set_yticks(token_ticks)
        ax.set_yticklabels(token_ticks)
        
        # è®¾ç½®åˆ»åº¦å­—ä½“å¤§å°ï¼Œå‚è€ƒdraw_chunk_runtime
        ax.tick_params(axis='both', pad=8, labelsize=19)  # å‚è€ƒdraw_chunk_runtime
        
        # è®¾ç½®ç½‘æ ¼ï¼Œå‚è€ƒdraw_chunk_runtime
        ax.grid(True, linestyle='--', alpha=0.3, linewidth=2)  # å‚è€ƒdraw_chunk_runtime
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.02)
        cbar.set_label('Model Run Time (ms)', fontsize=16, labelpad=5)
        cbar.ax.tick_params(labelsize=14)
        
        # è°ƒæ•´å¸ƒå±€
        plt.subplots_adjust(left=0.12, right=0.85, bottom=0.15, top=0.92)
        
        # ä¿å­˜å›¾è¡¨
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            if self.verbose:
                print(f"âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        return fig
    
    def run_end_to_end(self, log_path: str, save_path: str = None, 
                      resolution: tuple = (25, 25)) -> bool:
        """ç«¯åˆ°ç«¯è¿è¡Œçƒ­åŠ›å›¾ç”Ÿæˆ"""
        try:
            if self.verbose:
                print("ğŸš€ å¼€å§‹ç”ŸæˆBatch Size vs Total Tokensçƒ­åŠ›å›¾")
                print("=" * 60)
            
            # 1. è¯»å–æ•°æ®
            df = self.read_profiling_data(log_path)
            if df is None or len(df) == 0:
                if self.verbose:
                    print("âŒ æ— æ³•è¯»å–æœ‰æ•ˆæ•°æ®")
                return False
            
            # 2. æå–ç‰¹å¾
            valid_df = self.extract_batch_and_token_features(df)
            if len(valid_df) < 10:
                if self.verbose:
                    print("âš ï¸ æœ‰æ•ˆæ•°æ®ç‚¹å¤ªå°‘ï¼Œå¯èƒ½å½±å“å›¾è¡¨è´¨é‡")
            
            # 3. åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
            runtime_matrix, batch_grid, token_grid, fitted_params, r_squared = self.create_heatmap_data(
                valid_df, resolution=resolution
            )
            
            # 4. ç»˜åˆ¶çƒ­åŠ›å›¾
            fig = self.plot_heatmap(
                runtime_matrix, batch_grid, token_grid, fitted_params, r_squared, save_path
            )
            
            if self.verbose:
                print("\nğŸ‰ çƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆï¼")
                print("=" * 60)
                print("ğŸ“ æ¨¡å‹æ‹Ÿåˆç»“æœ:")
                print(f"   RÂ² = {r_squared:.4f}")
                print(f"   çº¿æ€§ç³»æ•° (Î±): {fitted_params['alpha']:.6f}")
                print(f"   é¥±å’Œæœ€å¤§å€¼ (Î³): {fitted_params['gamma']:.2f}")
                print(f"   é¥±å’Œå‚æ•° (B0): {fitted_params['B0']:.2f}")
                print(f"ğŸ“Š æ¨¡å‹å…¬å¼: T(B,S) = {fitted_params['base_time']:.2f} + {fitted_params['alpha']:.6f}Ã—S + {fitted_params['gamma']:.2f}Ã—(1-e^(-B/{fitted_params['B0']:.2f}))")
            
            plt.show()
            return True
            
        except Exception as e:
            if self.verbose:
                print(f"âŒ ç”Ÿæˆçƒ­åŠ›å›¾æ—¶å‡ºé”™: {e}")
                traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”ŸæˆBatch Size vs Total Tokensçƒ­åŠ›å›¾')
    parser.add_argument('log_path', type=str, nargs='?', 
                       default='scheduler_profiling.jsonl',
                       help='profilingæ•°æ®æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--save-path', type=str, 
                       default="./paper_figs/batch_token_heatmap_fig/batch_token_heatmap.pdf",
                       help='å›¾è¡¨ä¿å­˜è·¯å¾„')
    parser.add_argument('--resolution', type=int, nargs=2, default=[10, 10],
                       help='çƒ­åŠ›å›¾åˆ†è¾¨ç‡ (batch_bins token_bins)')
    parser.add_argument('--verbose', action='store_true', default=True,
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    generator = BatchTokenHeatmapGenerator(verbose=args.verbose)
    
    success = generator.run_end_to_end(
        log_path=args.log_path,
        save_path=args.save_path,
        resolution=tuple(args.resolution)
    )
    
    if success:
        print("âœ… çƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆï¼")
    else:
        print("âŒ çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥ï¼")
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main() 