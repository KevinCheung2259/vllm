#!/usr/bin/env python3
"""
åˆ›å»ºé¢„æ‹Ÿåˆæ¨¡å‹çš„å·¥å…·è„šæœ¬

è¯¥è„šæœ¬ç”¨äºä»å†å²profilingæ•°æ®è®­ç»ƒä¸€ä¸ªæ€§èƒ½æ¨¡å‹ï¼Œå¹¶ä¿å­˜ä¸ºé¢„æ‹Ÿåˆæ¨¡å‹æ–‡ä»¶ï¼Œ
ä¾›SLAè°ƒåº¦å™¨ç›´æ¥ä½¿ç”¨ï¼Œé¿å…å†·å¯åŠ¨é—®é¢˜ã€‚
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import argparse
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ å½“å‰æ¨¡å—è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from throughput_model import ThroughputSaturationModel
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_profiling_data(data_path: str) -> pd.DataFrame:
    """ä»profilingæ–‡ä»¶åŠ è½½æ•°æ®
    
    Args:
        data_path: profilingæ•°æ®è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        
    Returns:
        åˆå¹¶åçš„DataFrame
    """
    data_path = Path(data_path)
    data = []
    
    if data_path.is_file():
        # å•ä¸ªæ–‡ä»¶
        files = [data_path]
    elif data_path.is_dir():
        # ç›®å½•ä¸­çš„æ‰€æœ‰jsonlæ–‡ä»¶
        files = list(data_path.glob('*.jsonl'))
        if not files:
            raise ValueError(f"No jsonl files found in directory: {data_path}")
    else:
        raise ValueError(f"Path not found: {data_path}")
    
    logger.info(f"Loading data from {len(files)} file(s)...")
    
    # è¯»å–æ‰€æœ‰æ–‡ä»¶
    for file_path in sorted(files):
        logger.info(f"Loading: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
            # è·³è¿‡å‰åå‡ è¡Œï¼ˆå¯èƒ½ä¸ç¨³å®šï¼‰
            valid_lines = lines[10:-10] if len(lines) > 20 else lines
            
            for line in valid_lines:
                try:
                    entry = json.loads(line.strip())
                    data.append(entry)
                except json.JSONDecodeError:
                    continue
    
    if not data:
        raise ValueError("No valid data found")
    
    df = pd.DataFrame(data)
    logger.info(f"Loaded {len(df)} records")
    
    return df


def filter_data(df: pd.DataFrame) -> pd.DataFrame:
    """è¿‡æ»¤å’Œæ¸…ç†æ•°æ®
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        æ¸…ç†åçš„DataFrame
    """
    initial_count = len(df)
    
    # åŸºæœ¬è¿‡æ»¤
    df = df[df.get('schedule_duration_ms', 0) < 200]
    df = df[df.get('model_run_duration_ms', 0) < 200]
    df = df[df.get('model_run_duration_ms', 0) > 0]
    
    # éœ€è¦æœ‰valid chunk_sizes
    df = df[df['chunk_sizes'].notna()]
    
    logger.info(f"Data filtering: {initial_count} -> {len(df)} records")
    
    return df


def analyze_data(df: pd.DataFrame) -> Dict[str, Any]:
    """åˆ†ææ•°æ®ç‰¹å¾
    
    Args:
        df: æ•°æ®DataFrame
        
    Returns:
        æ•°æ®åˆ†æç»“æœ
    """
    def compute_batch_size(chunk_sizes):
        if isinstance(chunk_sizes, list):
            return len(chunk_sizes)
        return np.nan
    
    def compute_total_tokens(chunk_sizes):
        if isinstance(chunk_sizes, list) and len(chunk_sizes) > 0:
            return sum(chunk_sizes)
        return np.nan
    
    df['batch_size'] = df['chunk_sizes'].apply(compute_batch_size)
    df['total_tokens'] = df['chunk_sizes'].apply(compute_total_tokens)
    
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_mask = (
        df['batch_size'].notna() & 
        df['total_tokens'].notna() & 
        (df['batch_size'] > 0) & 
        (df['total_tokens'] > 0)
    )
    df_valid = df[valid_mask]
    
    analysis = {
        'total_records': len(df),
        'valid_records': len(df_valid),
        'batch_size_range': (df_valid['batch_size'].min(), df_valid['batch_size'].max()),
        'total_tokens_range': (df_valid['total_tokens'].min(), df_valid['total_tokens'].max()),
        'latency_range': (df_valid['model_run_duration_ms'].min(), df_valid['model_run_duration_ms'].max()),
        'batch_size_dist': df_valid['batch_size'].value_counts().head(10).to_dict(),
    }
    
    return analysis


def train_model(df: pd.DataFrame, verbose: bool = True) -> ThroughputSaturationModel:
    """è®­ç»ƒæ€§èƒ½æ¨¡å‹
    
    Args:
        df: è®­ç»ƒæ•°æ®
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    logger.info("Training throughput saturation model...")
    
    # åˆ›å»ºæ¨¡å‹
    model = ThroughputSaturationModel(verbose=verbose)
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(df)
    
    # æ‰“å°è®­ç»ƒç»“æœ
    summary = model.get_model_summary()
    logger.info(f"Model training completed:")
    logger.info(f"  RÂ² = {summary['metrics']['r2']:.4f}")
    logger.info(f"  RMSE = {summary['metrics']['rmse']:.3f} ms")
    logger.info(f"  MAE = {summary['metrics']['mae']:.3f} ms")
    logger.info(f"  Samples = {summary['metrics']['n_samples']}")
    
    return model


def create_demo_model(output_path: str) -> None:
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„æ¨¡å‹ï¼ˆå½“æ²¡æœ‰çœŸå®æ•°æ®æ—¶ï¼‰
    
    Args:
        output_path: è¾“å‡ºè·¯å¾„
    """
    logger.info("Creating demo model with synthetic data...")
    
    # ç”Ÿæˆåˆæˆæ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # æ¨¡æ‹Ÿçš„batch_sizeså’Œchunk_sizes
    batch_sizes = np.random.randint(1, 33, n_samples)
    chunk_sizes_list = []
    
    for b in batch_sizes:
        if np.random.random() < 0.3:  # 30% prefill
            sizes = np.random.randint(10, 200, b).tolist()
        else:  # 70% decode  
            sizes = [1] * b
        chunk_sizes_list.append(sizes)
    
    # ä½¿ç”¨çœŸå®çš„æ¨¡å‹å‚æ•°ç”Ÿæˆå»¶è¿Ÿ
    true_params = [50.0, 0.1, 0.02, 5.0, 0.05, 10.0, 0.5, 0.001]
    
    latencies = []
    for i, sizes in enumerate(chunk_sizes_list):
        B = len(sizes)
        S = sum(sizes)
        latency = ThroughputSaturationModel.latency_model(
            (np.array([B]), np.array([S])), *true_params
        )[0]
        latency += np.random.normal(0, latency * 0.1)  # 10% å™ªå£°
        latencies.append(max(latency, 1.0))
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'chunk_sizes': chunk_sizes_list,
        'model_run_duration_ms': latencies,
        'batch_id': range(n_samples)
    })
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(df, verbose=True)
    
    # ä¿å­˜æ¨¡å‹
    model.save_model(output_path)
    logger.info(f"Demo model saved to: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Create pretrained model for SLA scheduler')
    parser.add_argument('--data', type=str, help='Path to profiling data (file or directory)')
    parser.add_argument('--output', type=str, default='sla_scheduler_pretrained_model.pkl',
                       help='Output model file path')
    parser.add_argument('--demo', action='store_true', 
                       help='Create demo model with synthetic data')
    parser.add_argument('--analyze-only', action='store_true',
                       help='Only analyze data without training model')
    parser.add_argument('--verbose', action='store_true', default=True,
                       help='Verbose output')
    
    args = parser.parse_args()
    
    print("ğŸš€ SLAè°ƒåº¦å™¨é¢„æ‹Ÿåˆæ¨¡å‹åˆ›å»ºå·¥å…·")
    print("=" * 60)
    
    if args.demo:
        # åˆ›å»ºæ¼”ç¤ºæ¨¡å‹
        create_demo_model(args.output)
        return
    
    if not args.data:
        print("âŒ é”™è¯¯ï¼šéœ€è¦æä¾›æ•°æ®è·¯å¾„ (--data) æˆ–ä½¿ç”¨ --demo åˆ›å»ºæ¼”ç¤ºæ¨¡å‹")
        parser.print_help()
        return
    
    try:
        # åŠ è½½æ•°æ®
        df = load_profiling_data(args.data)
        
        # è¿‡æ»¤æ•°æ®
        df = filter_data(df)
        
        # åˆ†ææ•°æ®
        analysis = analyze_data(df)
        
        print("\nğŸ“Š æ•°æ®åˆ†æç»“æœ:")
        print(f"  æ€»è®°å½•æ•°: {analysis['total_records']}")
        print(f"  æœ‰æ•ˆè®°å½•æ•°: {analysis['valid_records']}")
        print(f"  Batch SizeèŒƒå›´: {analysis['batch_size_range']}")
        print(f"  Total TokensèŒƒå›´: {analysis['total_tokens_range']}")
        print(f"  å»¶è¿ŸèŒƒå›´: {analysis['latency_range']} ms")
        
        print(f"\nğŸ“ˆ Batch Sizeåˆ†å¸ƒ:")
        for bs, count in analysis['batch_size_dist'].items():
            print(f"    Batch {bs}: {count} æ¬¡")
        
        if args.analyze_only:
            print("\nâœ… æ•°æ®åˆ†æå®Œæˆï¼ˆä»…åˆ†ææ¨¡å¼ï¼‰")
            return
        
        if analysis['valid_records'] < 50:
            print(f"\nâš ï¸  è­¦å‘Šï¼šæœ‰æ•ˆæ ·æœ¬æ•°è¿‡å°‘ ({analysis['valid_records']})ï¼Œå»ºè®®è‡³å°‘50ä¸ªæ ·æœ¬")
            response = input("æ˜¯å¦ç»§ç»­è®­ç»ƒæ¨¡å‹ï¼Ÿ (y/N): ")
            if response.lower() != 'y':
                return
        
        # è®­ç»ƒæ¨¡å‹
        print(f"\nğŸ”§ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        model = train_model(df, verbose=args.verbose)
        
        # ä¿å­˜æ¨¡å‹
        model.save_model(args.output)
        
        print(f"\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜åˆ°: {args.output}")
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"   export VLLM_SLA_USE_PRETRAINED=true")
        print(f"   export VLLM_SLA_PRETRAINED_PATH='{args.output}'")
        print(f"   # ç„¶åå¯åŠ¨vLLM")
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
