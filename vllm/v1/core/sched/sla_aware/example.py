#!/usr/bin/env python3
"""
SLAæ„ŸçŸ¥è°ƒåº¦å™¨ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•é…ç½®å’Œä½¿ç”¨SLAæ„ŸçŸ¥è°ƒåº¦å™¨ï¼ŒåŒ…æ‹¬æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•ã€‚
"""

import os
import time
from typing import List, Dict, Any

# è®¾ç½®ç¤ºä¾‹é…ç½®
def setup_example_config():
    """è®¾ç½®SLAè°ƒåº¦å™¨ç¤ºä¾‹é…ç½®"""
    os.environ['VLLM_SLA_SCHEDULER_ENABLED'] = 'true'
    os.environ['VLLM_SLO_TPOT_MS'] = '50.0'
    os.environ['VLLM_SLA_MIN_BATCH_TIME_MS'] = '15.0'
    os.environ['VLLM_SLA_QUEUE_THRESHOLD'] = '5'
    os.environ['VLLM_SLA_VERBOSE'] = 'true'
    os.environ['VLLM_SLA_FALLBACK_ON_ERROR'] = 'true'
    
    print("âœ… SLAè°ƒåº¦å™¨é…ç½®å·²è®¾ç½®")


def demonstrate_config_loading():
    """æ¼”ç¤ºé…ç½®åŠ è½½"""
    print("\nğŸ“‹ é…ç½®åŠ è½½ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        from .config import SLASchedulerConfig
        
        # ä»ç¯å¢ƒå˜é‡åŠ è½½
        config = SLASchedulerConfig.from_env()
        print(f"ä»ç¯å¢ƒå˜é‡åŠ è½½çš„é…ç½®: {config}")
        
        # ç¨‹åºåŒ–é…ç½®
        custom_config = SLASchedulerConfig(
            enabled=True,
            slo_tpot_ms=60.0,
            min_batch_time_ms=20.0,
            queue_threshold=3,
            verbose_logging=True
        )
        print(f"è‡ªå®šä¹‰é…ç½®: {custom_config}")
        
        return config
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥é…ç½®æ¨¡å—: {e}")
        return None


def demonstrate_performance_predictor():
    """æ¼”ç¤ºæ€§èƒ½é¢„æµ‹å™¨"""
    print("\nğŸ”® æ€§èƒ½é¢„æµ‹å™¨ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        from .config import SLASchedulerConfig
        from .performance_predictor import PerformancePredictor
        
        config = SLASchedulerConfig(verbose_logging=True)
        predictor = PerformancePredictor(config)
        
        print("ğŸ“Š æ·»åŠ æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®...")
        
        # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿæ•°æ®
        sample_data = [
            (4, 64, 12.5),    # batch=4, tokens=64, latency=12.5ms
            (8, 128, 18.2),   # batch=8, tokens=128, latency=18.2ms
            (16, 256, 28.7),  # batch=16, tokens=256, latency=28.7ms
            (32, 512, 45.3),  # batch=32, tokens=512, latency=45.3ms
        ]
        
        for batch_size, tokens, latency in sample_data:
            predictor.add_observation(batch_size, tokens, latency)
            print(f"  æ·»åŠ è§‚æµ‹: B={batch_size}, S={tokens}, T={latency}ms")
        
        print("\nğŸ¯ é¢„æµ‹æµ‹è¯•...")
        test_cases = [(8, 200), (16, 400), (24, 600)]
        
        for batch_size, tokens in test_cases:
            pred_latency = predictor.predict_latency(batch_size, tokens)
            token_budget = predictor.solve_for_token_budget(batch_size, 30.0)
            
            print(f"  B={batch_size}, S={tokens} -> é¢„æµ‹å»¶è¿Ÿ: {pred_latency:.2f}ms")
            print(f"  B={batch_size}, ç›®æ ‡30ms -> Tokené¢„ç®—: {token_budget}")
        
        status = predictor.get_status()
        print(f"\nğŸ“ˆ é¢„æµ‹å™¨çŠ¶æ€: {status}")
        
        return predictor
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥é¢„æµ‹å™¨æ¨¡å—: {e}")
        return None


def demonstrate_sla_scheduler():
    """æ¼”ç¤ºSLAè°ƒåº¦å™¨ä¸»æ¥å£"""
    print("\nğŸ›ï¸ SLAè°ƒåº¦å™¨ä¸»æ¥å£ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        from .sla_scheduler import SLAScheduler
        
        scheduler = SLAScheduler()
        print(f"âœ… SLAè°ƒåº¦å™¨å·²åˆå§‹åŒ–: {scheduler.enabled}")
        
        # æ¨¡æ‹Ÿè°ƒåº¦è¯·æ±‚
        print("\nğŸ“‹ æ¨¡æ‹Ÿè°ƒåº¦è®¡ç®—...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿè¯·æ±‚ï¼ˆè¿™é‡Œç”¨ç®€åŒ–çš„æ•°æ®ç»“æ„ï¼‰
        class MockRequest:
            def __init__(self, req_id: str, num_computed_tokens: int = 0, num_prompt_tokens: int = 100):
                self.request_id = req_id
                self.num_computed_tokens = num_computed_tokens
                self.num_prompt_tokens = num_prompt_tokens
        
        running_requests = [
            MockRequest("req_1", 50, 100),   # prefillé˜¶æ®µ
            MockRequest("req_2", 100, 100),  # decodeé˜¶æ®µ
            MockRequest("req_3", 100, 100),  # decodeé˜¶æ®µ
        ]
        
        waiting_requests = [
            MockRequest("req_4", 0, 200),
            MockRequest("req_5", 0, 150),
        ]
        
        # è®¡ç®—tokené¢„ç®—å’Œç›®æ ‡å»¶è¿Ÿ
        token_budget, target_latency = scheduler.compute_token_budget_and_target(
            running_requests=running_requests,
            waiting_requests=waiting_requests,
            max_tokens=512,
            max_batch_size=32
        )
        
        print(f"  Tokené¢„ç®—: {token_budget}")
        print(f"  ç›®æ ‡å»¶è¿Ÿ: {target_latency:.2f}ms")
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¼˜å…ˆdecode
        prioritize_decode = scheduler.should_prioritize_decode(running_requests)
        print(f"  ä¼˜å…ˆdecode: {prioritize_decode}")
        
        # æ¨¡æ‹Ÿæ€§èƒ½è®°å½•
        scheduler.record_performance(5, 320, 35.5)
        print(f"  è®°å½•æ€§èƒ½: B=5, S=320, T=35.5ms")
        
        # è·å–çŠ¶æ€
        status = scheduler.get_status()
        print(f"\nğŸ“Š è°ƒåº¦å™¨çŠ¶æ€:")
        for key, value in status.items():
            if key != 'predictor':  # ç®€åŒ–è¾“å‡º
                print(f"  {key}: {value}")
        
        simple_status = scheduler.get_simple_status()
        print(f"\nğŸ“Š ç®€åŒ–çŠ¶æ€: {simple_status}")
        
        return scheduler
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥SLAè°ƒåº¦å™¨æ¨¡å—: {e}")
        return None


def demonstrate_integration():
    """æ¼”ç¤ºä¸ä¸»è°ƒåº¦å™¨çš„é›†æˆ"""
    print("\nğŸ”— è°ƒåº¦å™¨é›†æˆç¤ºä¾‹")
    print("=" * 50)
    
    print("SLAè°ƒåº¦å™¨ä¸vLLMä¸»è°ƒåº¦å™¨çš„é›†æˆç‚¹:")
    print("1. åœ¨Scheduler.__init__()ä¸­åˆå§‹åŒ–SLAè°ƒåº¦å™¨")
    print("2. åœ¨schedule()æ–¹æ³•ä¸­è°ƒç”¨compute_token_budget_and_target()")
    print("3. åœ¨_finalize_and_log_profiling()ä¸­è®°å½•æ€§èƒ½æ•°æ®")
    print("4. é€šè¿‡get_sla_scheduler_status()ç›‘æ§çŠ¶æ€")
    
    print("\né›†æˆä»£ç ç¤ºä¾‹:")
    print("""
    # åœ¨ä¸»è°ƒåº¦å™¨ä¸­
    if self.sla_scheduler and self.sla_scheduler.enabled:
        token_budget, target_latency = self.sla_scheduler.compute_token_budget_and_target(
            running_requests=self.running,
            waiting_requests=list(self.waiting),
            max_tokens=self.max_num_scheduled_tokens,
            max_batch_size=self.max_num_running_reqs
        )
    else:
        # å›é€€åˆ°åŸæœ‰é€»è¾‘
        token_budget = self.max_num_scheduled_tokens
        target_latency = self.slo_tpot_ms
    """)


def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    try:
        from .performance_predictor import PerformancePredictor
        from .config import SLASchedulerConfig
        
        config = SLASchedulerConfig(verbose_logging=False)
        predictor = PerformancePredictor(config)
        
        # é¢„çƒ­ï¼šæ·»åŠ ä¸€äº›æ•°æ®
        for i in range(100):
            batch_size = (i % 32) + 1
            tokens = (i % 512) + 64
            latency = 10 + 0.02 * tokens + 0.5 * batch_size
            predictor.add_observation(batch_size, tokens, latency)
        
        # åŸºå‡†æµ‹è¯•é¢„æµ‹æ€§èƒ½
        test_cases = [(8, 256), (16, 512), (32, 1024)]
        iterations = 1000
        
        for batch_size, tokens in test_cases:
            start_time = time.perf_counter()
            
            for _ in range(iterations):
                predictor.predict_latency(batch_size, tokens)
            
            end_time = time.perf_counter()
            avg_time_ms = (end_time - start_time) * 1000 / iterations
            
            print(f"é¢„æµ‹æ€§èƒ½ B={batch_size}, S={tokens}: {avg_time_ms:.4f}ms/æ¬¡")
        
        # åŸºå‡†æµ‹è¯•tokenæ±‚è§£æ€§èƒ½
        start_time = time.perf_counter()
        for _ in range(iterations):
            predictor.solve_for_token_budget(16, 30.0)
        end_time = time.perf_counter()
        avg_solve_time_ms = (end_time - start_time) * 1000 / iterations
        
        print(f"Tokenæ±‚è§£æ€§èƒ½: {avg_solve_time_ms:.4f}ms/æ¬¡")
        
        print(f"\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œé¢„æµ‹å¼€é”€è¿œå°äº1msé™åˆ¶")
        
    except ImportError as e:
        print(f"âŒ æ— æ³•è¿è¡Œæ€§èƒ½æµ‹è¯•: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ SLAæ„ŸçŸ¥è°ƒåº¦å™¨ç¤ºä¾‹ç¨‹åº")
    print("=" * 60)
    
    # 1. è®¾ç½®é…ç½®
    setup_example_config()
    
    # 2. æ¼”ç¤ºé…ç½®åŠ è½½
    config = demonstrate_config_loading()
    
    # 3. æ¼”ç¤ºæ€§èƒ½é¢„æµ‹å™¨
    predictor = demonstrate_performance_predictor()
    
    # 4. æ¼”ç¤ºSLAè°ƒåº¦å™¨
    scheduler = demonstrate_sla_scheduler()
    
    # 5. æ¼”ç¤ºé›†æˆ
    demonstrate_integration()
    
    # 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
    run_performance_benchmark()
    
    print("\nğŸ‰ ç¤ºä¾‹ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("\nè¦åœ¨å®é™…vLLMä¸­ä½¿ç”¨SLAè°ƒåº¦å™¨ï¼Œè¯·è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡ï¼Œ")
    print("æˆ–å‚è€ƒREADME.mdä¸­çš„è¯¦ç»†é…ç½®è¯´æ˜ã€‚")


if __name__ == '__main__':
    main()
